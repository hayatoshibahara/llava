{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bitsandbytes sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7f42d",
   "metadata": {},
   "source": [
    "### Vision Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aca916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n",
    "\n",
    "class CLIPVisionTower(nn.Module):\n",
    "    def __init__(self, vision_tower, args, delay_load=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_loaded = False\n",
    "\n",
    "        self.vision_tower_name = vision_tower\n",
    "        self.select_layer = args.mm_vision_select_layer\n",
    "        self.select_feature = getattr(args, 'mm_vision_select_feature', 'patch')\n",
    "\n",
    "        if not delay_load:\n",
    "            self.load_model()\n",
    "        elif getattr(args, 'unfreeze_mm_vision_tower', False):\n",
    "            self.load_model()\n",
    "        else:\n",
    "            self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n",
    "\n",
    "    def load_model(self, device_map=None):\n",
    "        if self.is_loaded:\n",
    "            print('{} is already loaded, `load_model` called again, skipping.'.format(self.vision_tower_name))\n",
    "            return\n",
    "\n",
    "        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)\n",
    "        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)\n",
    "        self.vision_tower.requires_grad_(False)\n",
    "\n",
    "        self.is_loaded = True\n",
    "\n",
    "    def feature_select(self, image_forward_outs):\n",
    "        image_features = image_forward_outs.hidden_states[self.select_layer]\n",
    "        if self.select_feature == 'patch':\n",
    "            image_features = image_features[:, 1:]\n",
    "        elif self.select_feature == 'cls_patch':\n",
    "            image_features = image_features\n",
    "        else:\n",
    "            raise ValueError(f'Unexpected select feature: {self.select_feature}')\n",
    "        return image_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, images):\n",
    "        if type(images) is list:\n",
    "            image_features = []\n",
    "            for image in images:\n",
    "                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n",
    "                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n",
    "                image_features.append(image_feature)\n",
    "        else:\n",
    "            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)\n",
    "            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    @property\n",
    "    def dummy_feature(self):\n",
    "        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.vision_tower.dtype\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.vision_tower.device\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        if self.is_loaded:\n",
    "            return self.vision_tower.config\n",
    "        else:\n",
    "            return self.cfg_only\n",
    "\n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        return self.config.hidden_size\n",
    "\n",
    "    @property\n",
    "    def num_patches_per_side(self):\n",
    "        return self.config.image_size // self.config.patch_size\n",
    "\n",
    "    @property\n",
    "    def num_patches(self):\n",
    "        return (self.config.image_size // self.config.patch_size) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vision_tower(vision_tower_cfg, **kwargs):\n",
    "    vision_tower = getattr(vision_tower_cfg, 'mm_vision_tower', getattr(vision_tower_cfg, 'vision_tower', None))\n",
    "    is_absolute_path_exists = os.path.exists(vision_tower)\n",
    "    use_s2 = getattr(vision_tower_cfg, 's2', False)\n",
    "    if is_absolute_path_exists or vision_tower.startswith(\"openai\") or vision_tower.startswith(\"laion\") or \"ShareGPT4V\" in vision_tower:\n",
    "\n",
    "        # False\n",
    "        if use_s2:\n",
    "            print(\"Using CLIPVisionTowerS2 as vision tower.\")\n",
    "            return CLIPVisionTowerS2(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "        else:\n",
    "            print(\"Using CLIPVisionTower as vision tower.\")\n",
    "            return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "\n",
    "    raise ValueError(f'Unknown vision tower: {vision_tower}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8b3e2",
   "metadata": {},
   "source": [
    "### Vision Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "\n",
    "def build_vision_projector(config, delay_load=False, **kwargs):\n",
    "    projector_type = getattr(config, 'mm_projector_type', 'linear')\n",
    "\n",
    "    # False\n",
    "    if projector_type == 'linear':\n",
    "        print(\"Using linear projector for vision features.\")\n",
    "        return nn.Linear(config.mm_hidden_size, config.hidden_size)\n",
    "\n",
    "    mlp_gelu_match = re.match(r'^mlp(\\d+)x_gelu$', projector_type)\n",
    "\n",
    "    # True\n",
    "    if mlp_gelu_match:\n",
    "        print(f\"Using MLP projector with depth {mlp_gelu_match.group(1)} and GELU activations for vision features.\")\n",
    "        mlp_depth = int(mlp_gelu_match.group(1))\n",
    "        modules = [nn.Linear(config.mm_hidden_size, config.hidden_size)]\n",
    "        for _ in range(1, mlp_depth):\n",
    "            modules.append(nn.GELU())\n",
    "            modules.append(nn.Linear(config.hidden_size, config.hidden_size))\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    if projector_type == 'identity':\n",
    "        print(\"Using identity projector for vision features.\")\n",
    "        return IdentityMap()\n",
    "\n",
    "    raise ValueError(f'Unknown projector type: {projector_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd227f",
   "metadata": {},
   "source": [
    "### LlavaMetaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaMetaModel:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LlavaMetaModel, self).__init__(config)\n",
    "\n",
    "        if hasattr(config, \"mm_vision_tower\"):\n",
    "            self.vision_tower = build_vision_tower(config, delay_load=True)\n",
    "            self.mm_projector = build_vision_projector(config)\n",
    "\n",
    "            if 'unpad' in getattr(config, 'mm_patch_merge_type', ''):\n",
    "                self.image_newline = nn.Parameter(\n",
    "                    torch.empty(config.hidden_size, dtype=self.dtype)\n",
    "                )\n",
    "\n",
    "    def get_vision_tower(self):\n",
    "        vision_tower = getattr(self, 'vision_tower', None)\n",
    "        if type(vision_tower) is list:\n",
    "            vision_tower = vision_tower[0]\n",
    "        return vision_tower\n",
    "\n",
    "    def initialize_vision_modules(self, model_args, fsdp=None):\n",
    "        vision_tower = model_args.vision_tower\n",
    "        mm_vision_select_layer = model_args.mm_vision_select_layer\n",
    "        mm_vision_select_feature = model_args.mm_vision_select_feature\n",
    "        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n",
    "        mm_patch_merge_type = model_args.mm_patch_merge_type\n",
    "\n",
    "        self.config.mm_vision_tower = vision_tower\n",
    "\n",
    "        if self.get_vision_tower() is None:\n",
    "            vision_tower = build_vision_tower(model_args)\n",
    "\n",
    "            if fsdp is not None and len(fsdp) > 0:\n",
    "                self.vision_tower = [vision_tower]\n",
    "            else:\n",
    "                self.vision_tower = vision_tower\n",
    "        else:\n",
    "            if fsdp is not None and len(fsdp) > 0:\n",
    "                vision_tower = self.vision_tower[0]\n",
    "            else:\n",
    "                vision_tower = self.vision_tower\n",
    "            vision_tower.load_model()\n",
    "\n",
    "        self.config.use_mm_proj = True\n",
    "        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n",
    "        self.config.mm_hidden_size = vision_tower.hidden_size\n",
    "        self.config.mm_vision_select_layer = mm_vision_select_layer\n",
    "        self.config.mm_vision_select_feature = mm_vision_select_feature\n",
    "        self.config.mm_patch_merge_type = mm_patch_merge_type\n",
    "\n",
    "        if getattr(self, 'mm_projector', None) is None:\n",
    "            self.mm_projector = build_vision_projector(self.config)\n",
    "\n",
    "            if 'unpad' in mm_patch_merge_type:\n",
    "                embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))\n",
    "                self.image_newline = nn.Parameter(\n",
    "                    torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std\n",
    "                )\n",
    "        else:\n",
    "            # In case it is frozen by LoRA\n",
    "            for p in self.mm_projector.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        if pretrain_mm_mlp_adapter is not None:\n",
    "            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n",
    "            def get_w(weights, keyword):\n",
    "                return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n",
    "\n",
    "            self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efab233",
   "metadata": {},
   "source": [
    "### LlavaMetaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class LlavaMetaForCausalLM(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_model(self):\n",
    "        pass\n",
    "\n",
    "    def get_vision_tower(self):\n",
    "        return self.get_model().get_vision_tower()\n",
    "\n",
    "    def encode_images(self, images):\n",
    "        image_features = self.get_model().get_vision_tower()(images)\n",
    "        image_features = self.get_model().mm_projector(image_features)\n",
    "        return image_features\n",
    "\n",
    "    def prepare_inputs_labels_for_multimodal(\n",
    "        self, input_ids, position_ids, attention_mask, past_key_values, labels,\n",
    "        images, image_sizes=None\n",
    "    ):\n",
    "        vision_tower = self.get_vision_tower()\n",
    "        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n",
    "            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n",
    "\n",
    "        if type(images) is list or images.ndim == 5:\n",
    "            if type(images) is list:\n",
    "                images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images]\n",
    "            concat_images = torch.cat([image for image in images], dim=0)\n",
    "            image_features = self.encode_images(concat_images)\n",
    "            split_sizes = [image.shape[0] for image in images]\n",
    "            image_features = torch.split(image_features, split_sizes, dim=0)\n",
    "            mm_patch_merge_type = getattr(self.config, 'mm_patch_merge_type', 'flat')\n",
    "            image_aspect_ratio = getattr(self.config, 'image_aspect_ratio', 'square')\n",
    "            if mm_patch_merge_type == 'flat':\n",
    "                image_features = [x.flatten(0, 1) for x in image_features]\n",
    "            elif mm_patch_merge_type.startswith('spatial'):\n",
    "                new_image_features = []\n",
    "                for image_idx, image_feature in enumerate(image_features):\n",
    "                    if image_feature.shape[0] > 1:\n",
    "                        base_image_feature = image_feature[0]\n",
    "                        image_feature = image_feature[1:]\n",
    "                        height = width = self.get_vision_tower().num_patches_per_side\n",
    "                        assert height * width == base_image_feature.shape[0]\n",
    "                        if image_aspect_ratio == 'anyres':\n",
    "                            num_patch_width, num_patch_height = get_anyres_image_grid_shape(image_sizes[image_idx], self.config.image_grid_pinpoints, self.get_vision_tower().config.image_size)\n",
    "                            image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n",
    "                        else:\n",
    "                            raise NotImplementedError\n",
    "                        if 'unpad' in mm_patch_merge_type:\n",
    "                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n",
    "                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n",
    "                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n",
    "                            image_feature = torch.cat((\n",
    "                                image_feature,\n",
    "                                self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)\n",
    "                            ), dim=-1)\n",
    "                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n",
    "                        else:\n",
    "                            image_feature = image_feature.permute(0, 2, 1, 3, 4).contiguous()\n",
    "                            image_feature = image_feature.flatten(0, 3)\n",
    "                        image_feature = torch.cat((base_image_feature, image_feature), dim=0)\n",
    "                    else:\n",
    "                        image_feature = image_feature[0]\n",
    "                        if 'unpad' in mm_patch_merge_type:\n",
    "                            image_feature = torch.cat((\n",
    "                                image_feature,\n",
    "                                self.model.image_newline[None].to(image_feature.device)\n",
    "                            ), dim=0)\n",
    "                    new_image_features.append(image_feature)\n",
    "                image_features = new_image_features\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected mm_patch_merge_type: {self.config.mm_patch_merge_type}\")\n",
    "        else:\n",
    "            image_features = self.encode_images(images)\n",
    "\n",
    "        # TODO: image start / end is not implemented here to support pretraining.\n",
    "        if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_im_start_end', False):\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Let's just add dummy tensors if they do not exist,\n",
    "        # it is a headache to deal with None all the time.\n",
    "        # But it is not ideal, and if you have a better idea,\n",
    "        # please open an issue / submit a PR, thanks.\n",
    "        _labels = labels\n",
    "        _position_ids = position_ids\n",
    "        _attention_mask = attention_mask\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "        else:\n",
    "            attention_mask = attention_mask.bool()\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
    "        if labels is None:\n",
    "            labels = torch.full_like(input_ids, IGNORE_INDEX)\n",
    "\n",
    "        # remove the padding using attention_mask -- FIXME\n",
    "        _input_ids = input_ids\n",
    "        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n",
    "        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n",
    "\n",
    "        new_input_embeds = []\n",
    "        new_labels = []\n",
    "        cur_image_idx = 0\n",
    "        for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n",
    "            if num_images == 0:\n",
    "                cur_image_features = image_features[cur_image_idx]\n",
    "                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n",
    "                cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n",
    "                new_input_embeds.append(cur_input_embeds)\n",
    "                new_labels.append(labels[batch_idx])\n",
    "                cur_image_idx += 1\n",
    "                continue\n",
    "\n",
    "            image_token_indices = [-1] + torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + [cur_input_ids.shape[0]]\n",
    "            cur_input_ids_noim = []\n",
    "            cur_labels = labels[batch_idx]\n",
    "            cur_labels_noim = []\n",
    "            for i in range(len(image_token_indices) - 1):\n",
    "                cur_input_ids_noim.append(cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "                cur_labels_noim.append(cur_labels[image_token_indices[i]+1:image_token_indices[i+1]])\n",
    "            split_sizes = [x.shape[0] for x in cur_labels_noim]\n",
    "            cur_input_embeds = self.get_model().embed_tokens(torch.cat(cur_input_ids_noim))\n",
    "            cur_input_embeds_no_im = torch.split(cur_input_embeds, split_sizes, dim=0)\n",
    "            cur_new_input_embeds = []\n",
    "            cur_new_labels = []\n",
    "\n",
    "            for i in range(num_images + 1):\n",
    "                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n",
    "                cur_new_labels.append(cur_labels_noim[i])\n",
    "                if i < num_images:\n",
    "                    cur_image_features = image_features[cur_image_idx]\n",
    "                    cur_image_idx += 1\n",
    "                    cur_new_input_embeds.append(cur_image_features)\n",
    "                    cur_new_labels.append(torch.full((cur_image_features.shape[0],), IGNORE_INDEX, device=cur_labels.device, dtype=cur_labels.dtype))\n",
    "\n",
    "            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n",
    "\n",
    "            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n",
    "            cur_new_labels = torch.cat(cur_new_labels)\n",
    "\n",
    "            new_input_embeds.append(cur_new_input_embeds)\n",
    "            new_labels.append(cur_new_labels)\n",
    "\n",
    "        # Truncate sequences to max length as image embeddings can make the sequence longer\n",
    "        tokenizer_model_max_length = getattr(self.config, 'tokenizer_model_max_length', None)\n",
    "        if tokenizer_model_max_length is not None:\n",
    "            new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]\n",
    "            new_labels = [x[:tokenizer_model_max_length] for x in new_labels]\n",
    "\n",
    "        # Combine them\n",
    "        max_len = max(x.shape[0] for x in new_input_embeds)\n",
    "        batch_size = len(new_input_embeds)\n",
    "\n",
    "        new_input_embeds_padded = []\n",
    "        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n",
    "        attention_mask = torch.zeros((batch_size, max_len), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "        position_ids = torch.zeros((batch_size, max_len), dtype=position_ids.dtype, device=position_ids.device)\n",
    "\n",
    "        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n",
    "            cur_len = cur_new_embed.shape[0]\n",
    "            if getattr(self.config, 'tokenizer_padding_side', 'right') == \"left\":\n",
    "                new_input_embeds_padded.append(torch.cat((\n",
    "                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device),\n",
    "                    cur_new_embed\n",
    "                ), dim=0))\n",
    "                if cur_len > 0:\n",
    "                    new_labels_padded[i, -cur_len:] = cur_new_labels\n",
    "                    attention_mask[i, -cur_len:] = True\n",
    "                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
    "            else:\n",
    "                new_input_embeds_padded.append(torch.cat((\n",
    "                    cur_new_embed,\n",
    "                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device)\n",
    "                ), dim=0))\n",
    "                if cur_len > 0:\n",
    "                    new_labels_padded[i, :cur_len] = cur_new_labels\n",
    "                    attention_mask[i, :cur_len] = True\n",
    "                    position_ids[i, :cur_len] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
    "\n",
    "        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n",
    "\n",
    "        if _labels is None:\n",
    "            new_labels = None\n",
    "        else:\n",
    "            new_labels = new_labels_padded\n",
    "\n",
    "        if _attention_mask is None:\n",
    "            attention_mask = None\n",
    "        else:\n",
    "            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n",
    "\n",
    "        if _position_ids is None:\n",
    "            position_ids = None\n",
    "\n",
    "        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\n",
    "\n",
    "    def initialize_vision_tokenizer(self, model_args, tokenizer):\n",
    "        if model_args.mm_use_im_patch_token:\n",
    "            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
    "            self.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        if model_args.mm_use_im_start_end:\n",
    "            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
    "            self.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "            if num_new_tokens > 0:\n",
    "                input_embeddings = self.get_input_embeddings().weight.data\n",
    "                output_embeddings = self.get_output_embeddings().weight.data\n",
    "\n",
    "                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
    "                    dim=0, keepdim=True)\n",
    "                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
    "                    dim=0, keepdim=True)\n",
    "\n",
    "                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "            if model_args.tune_mm_mlp_adapter:\n",
    "                for p in self.get_input_embeddings().parameters():\n",
    "                    p.requires_grad = True\n",
    "                for p in self.get_output_embeddings().parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "            if model_args.pretrain_mm_mlp_adapter:\n",
    "                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location='cpu')\n",
    "                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\n",
    "                assert num_new_tokens == 2\n",
    "                if input_embeddings.shape == embed_tokens_weight.shape:\n",
    "                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n",
    "                elif embed_tokens_weight.shape[0] == num_new_tokens:\n",
    "                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n",
    "        elif model_args.mm_use_im_patch_token:\n",
    "            if model_args.tune_mm_mlp_adapter:\n",
    "                for p in self.get_input_embeddings().parameters():\n",
    "                    p.requires_grad = False\n",
    "                for p in self.get_output_embeddings().parameters():\n",
    "                    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36974f74",
   "metadata": {},
   "source": [
    "### LlavaLlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, \\\n",
    "                         LlamaConfig, LlamaModel, LlamaForCausalLM\n",
    "\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers.generation.utils import GenerateOutput\n",
    "\n",
    "# from ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM\n",
    "\n",
    "\n",
    "class LlavaConfig(LlamaConfig):\n",
    "    model_type = \"llava_llama\"\n",
    "\n",
    "\n",
    "class LlavaLlamaModel(LlavaMetaModel, LlamaModel):\n",
    "    config_class = LlavaConfig\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super(LlavaLlamaModel, self).__init__(config)\n",
    "\n",
    "\n",
    "class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):\n",
    "    config_class = LlavaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LlamaForCausalLM, self).__init__(config)\n",
    "        self.model = LlavaLlamaModel(config)\n",
    "        self.pretraining_tp = config.pretraining_tp\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        images: Optional[torch.FloatTensor] = None,\n",
    "        image_sizes: Optional[List[List[int]]] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position=None, # https://github.com/huggingface/transformers/issues/29426\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            (\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                past_key_values,\n",
    "                inputs_embeds,\n",
    "                labels\n",
    "            ) = self.prepare_inputs_labels_for_multimodal(\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                past_key_values,\n",
    "                labels,\n",
    "                images,\n",
    "                image_sizes\n",
    "            )\n",
    "\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        inputs: Optional[torch.Tensor] = None,\n",
    "        images: Optional[torch.Tensor] = None,\n",
    "        image_sizes: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
    "        position_ids = kwargs.pop(\"position_ids\", None)\n",
    "        attention_mask = kwargs.pop(\"attention_mask\", None)\n",
    "        if \"inputs_embeds\" in kwargs:\n",
    "            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n",
    "\n",
    "        if images is not None:\n",
    "            (\n",
    "                inputs,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                _,\n",
    "                inputs_embeds,\n",
    "                _\n",
    "            ) = self.prepare_inputs_labels_for_multimodal(\n",
    "                inputs,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                None,\n",
    "                None,\n",
    "                images,\n",
    "                image_sizes=image_sizes\n",
    "            )\n",
    "        else:\n",
    "            inputs_embeds = self.get_model().embed_tokens(inputs)\n",
    "\n",
    "        return super().generate(\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None,\n",
    "                                      inputs_embeds=None, **kwargs):\n",
    "        images = kwargs.pop(\"images\", None)\n",
    "        image_sizes = kwargs.pop(\"image_sizes\", None)\n",
    "        inputs = super().prepare_inputs_for_generation(\n",
    "            input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs\n",
    "        )\n",
    "        if images is not None:\n",
    "            inputs['images'] = images\n",
    "        if image_sizes is not None:\n",
    "            inputs['image_sizes'] = image_sizes\n",
    "        return inputs\n",
    "\n",
    "AutoConfig.register(\"llava_llama\", LlavaConfig)\n",
    "AutoModelForCausalLM.register(LlavaConfig, LlavaLlamaForCausalLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d9da9",
   "metadata": {},
   "source": [
    "### ロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa26e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name_from_path(model_path):\n",
    "    model_path = model_path.strip(\"/\")\n",
    "    model_paths = model_path.split(\"/\")\n",
    "    if model_paths[-1].startswith('checkpoint-'):\n",
    "        return model_paths[-2] + \"_\" + model_paths[-1]\n",
    "    else:\n",
    "        return model_paths[-1]\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "import torch\n",
    "# from llava.model import *\n",
    "# from llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "\n",
    "\n",
    "def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", device=\"cuda\", use_flash_attn=False, **kwargs):\n",
    "    kwargs = {\"device_map\": device_map, **kwargs}\n",
    "\n",
    "    if device != \"cuda\":\n",
    "        kwargs['device_map'] = {\"\": device}\n",
    "\n",
    "    if load_8bit:\n",
    "        kwargs['load_in_8bit'] = True\n",
    "    elif load_4bit:\n",
    "        kwargs['load_in_4bit'] = True\n",
    "        kwargs['quantization_config'] = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4'\n",
    "        )\n",
    "    else:\n",
    "        kwargs['torch_dtype'] = torch.float16\n",
    "\n",
    "    if use_flash_attn:\n",
    "        kwargs['attn_implementation'] = 'flash_attention_2'\n",
    "\n",
    "    if 'llava' in model_name.lower():\n",
    "        # Load LLaVA model\n",
    "        if 'lora' in model_name.lower() and model_base is None:\n",
    "            warnings.warn('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')\n",
    "        if 'lora' in model_name.lower() and model_base is not None:\n",
    "            from llava.model.language_model.llava_llama import LlavaConfig\n",
    "            lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n",
    "            print('Loading LLaVA from base model...')\n",
    "            model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\n",
    "            token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\n",
    "            if model.lm_head.weight.shape[0] != token_num:\n",
    "                model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n",
    "                model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n",
    "\n",
    "            print('Loading additional LLaVA weights...')\n",
    "            if os.path.exists(os.path.join(model_path, 'non_lora_trainables.bin')):\n",
    "                non_lora_trainables = torch.load(os.path.join(model_path, 'non_lora_trainables.bin'), map_location='cpu')\n",
    "            else:\n",
    "                # this is probably from HF Hub\n",
    "                from huggingface_hub import hf_hub_download\n",
    "                def load_from_hf(repo_id, filename, subfolder=None):\n",
    "                    cache_file = hf_hub_download(\n",
    "                        repo_id=repo_id,\n",
    "                        filename=filename,\n",
    "                        subfolder=subfolder)\n",
    "                    return torch.load(cache_file, map_location='cpu')\n",
    "                non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')\n",
    "            non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\n",
    "            if any(k.startswith('model.model.') for k in non_lora_trainables):\n",
    "                non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\n",
    "            model.load_state_dict(non_lora_trainables, strict=False)\n",
    "\n",
    "            from peft import PeftModel\n",
    "            print('Loading LoRA weights...')\n",
    "            model = PeftModel.from_pretrained(model, model_path)\n",
    "            print('Merging LoRA weights...')\n",
    "            model = model.merge_and_unload()\n",
    "            print('Model is loaded...')\n",
    "        elif model_base is not None:\n",
    "            # this may be mm projector only\n",
    "            print('Loading LLaVA from base model...')\n",
    "            if 'mpt' in model_name.lower():\n",
    "                if not os.path.isfile(os.path.join(model_path, 'configuration_mpt.py')):\n",
    "                    shutil.copyfile(os.path.join(model_base, 'configuration_mpt.py'), os.path.join(model_path, 'configuration_mpt.py'))\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\n",
    "                cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "                model = LlavaMptForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n",
    "                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n",
    "                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\n",
    "\n",
    "            mm_projector_weights = torch.load(os.path.join(model_path, 'mm_projector.bin'), map_location='cpu')\n",
    "            mm_projector_weights = {k: v.to(torch.float16) for k, v in mm_projector_weights.items()}\n",
    "            model.load_state_dict(mm_projector_weights, strict=False)\n",
    "        else:\n",
    "            if 'mpt' in model_name.lower():\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "                model = LlavaMptForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n",
    "            elif 'mistral' in model_name.lower():\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "                model = LlavaMistralForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    **kwargs\n",
    "                )\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "                model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    **kwargs\n",
    "                )\n",
    "    else:\n",
    "        # Load language model\n",
    "        if model_base is not None:\n",
    "            # PEFT model\n",
    "            from peft import PeftModel\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\n",
    "            print(f\"Loading LoRA weights from {model_path}\")\n",
    "            model = PeftModel.from_pretrained(model, model_path)\n",
    "            print(f\"Merging weights\")\n",
    "            model = model.merge_and_unload()\n",
    "            print('Convert to FP16...')\n",
    "            model.to(torch.float16)\n",
    "        else:\n",
    "            use_fast = False\n",
    "            if 'mpt' in model_name.lower():\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n",
    "\n",
    "    image_processor = None\n",
    "\n",
    "    if 'llava' in model_name.lower():\n",
    "        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n",
    "        mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\n",
    "        if mm_use_im_patch_token:\n",
    "            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
    "        if mm_use_im_start_end:\n",
    "            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        vision_tower = model.get_vision_tower()\n",
    "        if not vision_tower.is_loaded:\n",
    "            vision_tower.load_model(device_map=device_map)\n",
    "        if device_map != 'auto':\n",
    "            vision_tower.to(device=device_map, dtype=torch.float16)\n",
    "        image_processor = vision_tower.image_processor\n",
    "\n",
    "    if hasattr(model.config, \"max_sequence_length\"):\n",
    "        context_len = model.config.max_sequence_length\n",
    "    else:\n",
    "        context_len = 2048\n",
    "\n",
    "    return tokenizer, model, image_processor, context_len\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a1a21",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROLLER_HEART_BEAT_EXPIRATION = 30\n",
    "WORKER_HEART_BEAT_INTERVAL = 15\n",
    "\n",
    "LOGDIR = \".\"\n",
    "\n",
    "# Model Constants\n",
    "IGNORE_INDEX = -100\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "IMAGE_PLACEHOLDER = \"<image-placeholder>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de25f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from enum import auto, Enum\n",
    "from typing import List, Tuple\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "class SeparatorStyle(Enum):\n",
    "    \"\"\"Different separator style.\"\"\"\n",
    "    SINGLE = auto()\n",
    "    TWO = auto()\n",
    "    MPT = auto()\n",
    "    PLAIN = auto()\n",
    "    LLAMA_2 = auto()\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Conversation:\n",
    "    \"\"\"A class that keeps all conversation history.\"\"\"\n",
    "    system: str\n",
    "    roles: List[str]\n",
    "    messages: List[List[str]]\n",
    "    offset: int\n",
    "    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n",
    "    sep: str = \"###\"\n",
    "    sep2: str = None\n",
    "    version: str = \"Unknown\"\n",
    "\n",
    "    skip_next: bool = False\n",
    "\n",
    "    def get_prompt(self):\n",
    "        messages = self.messages\n",
    "        if len(messages) > 0 and type(messages[0][1]) is tuple:\n",
    "            messages = self.messages.copy()\n",
    "            init_role, init_msg = messages[0].copy()\n",
    "            init_msg = init_msg[0].replace(\"<image>\", \"\").strip()\n",
    "            if 'mmtag' in self.version:\n",
    "                messages[0] = (init_role, init_msg)\n",
    "                messages.insert(0, (self.roles[0], \"<Image><image></Image>\"))\n",
    "                messages.insert(1, (self.roles[1], \"Received.\"))\n",
    "            else:\n",
    "                messages[0] = (init_role, \"<image>\\n\" + init_msg)\n",
    "\n",
    "        if self.sep_style == SeparatorStyle.SINGLE:\n",
    "            ret = self.system + self.sep\n",
    "            for role, message in messages:\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    ret += role + \": \" + message + self.sep\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "        elif self.sep_style == SeparatorStyle.TWO:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = self.system + seps[0]\n",
    "            for i, (role, message) in enumerate(messages):\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    ret += role + \": \" + message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "        elif self.sep_style == SeparatorStyle.MPT:\n",
    "            ret = self.system + self.sep\n",
    "            for role, message in messages:\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    ret += role + message + self.sep\n",
    "                else:\n",
    "                    ret += role\n",
    "        elif self.sep_style == SeparatorStyle.LLAMA_2:\n",
    "            wrap_sys = lambda msg: f\"<<SYS>>\\n{msg}\\n<</SYS>>\\n\\n\" if len(msg) > 0 else msg\n",
    "            wrap_inst = lambda msg: f\"[INST] {msg} [/INST]\"\n",
    "            ret = \"\"\n",
    "\n",
    "            for i, (role, message) in enumerate(messages):\n",
    "                if i == 0:\n",
    "                    assert message, \"first message should not be none\"\n",
    "                    assert role == self.roles[0], \"first message should come from user\"\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    if i == 0: message = wrap_sys(self.system) + message\n",
    "                    if i % 2 == 0:\n",
    "                        message = wrap_inst(message)\n",
    "                        ret += self.sep + message\n",
    "                    else:\n",
    "                        ret += \" \" + message + \" \" + self.sep2\n",
    "                else:\n",
    "                    ret += \"\"\n",
    "            ret = ret.lstrip(self.sep)\n",
    "        elif self.sep_style == SeparatorStyle.PLAIN:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = self.system\n",
    "            for i, (role, message) in enumerate(messages):\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    ret += message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += \"\"\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid style: {self.sep_style}\")\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def append_message(self, role, message):\n",
    "        self.messages.append([role, message])\n",
    "\n",
    "    def process_image(self, image, image_process_mode, return_pil=False, image_format='PNG', max_len=1344, min_len=672):\n",
    "        if image_process_mode == \"Pad\":\n",
    "            def expand2square(pil_img, background_color=(122, 116, 104)):\n",
    "                width, height = pil_img.size\n",
    "                if width == height:\n",
    "                    return pil_img\n",
    "                elif width > height:\n",
    "                    result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "                    result.paste(pil_img, (0, (width - height) // 2))\n",
    "                    return result\n",
    "                else:\n",
    "                    result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "                    result.paste(pil_img, ((height - width) // 2, 0))\n",
    "                    return result\n",
    "            image = expand2square(image)\n",
    "        elif image_process_mode in [\"Default\", \"Crop\"]:\n",
    "            pass\n",
    "        elif image_process_mode == \"Resize\":\n",
    "            image = image.resize((336, 336))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid image_process_mode: {image_process_mode}\")\n",
    "        if max(image.size) > max_len:\n",
    "            max_hw, min_hw = max(image.size), min(image.size)\n",
    "            aspect_ratio = max_hw / min_hw\n",
    "            shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n",
    "            longest_edge = int(shortest_edge * aspect_ratio)\n",
    "            W, H = image.size\n",
    "            if H > W:\n",
    "                H, W = longest_edge, shortest_edge\n",
    "            else:\n",
    "                H, W = shortest_edge, longest_edge\n",
    "            image = image.resize((W, H))\n",
    "        if return_pil:\n",
    "            return image\n",
    "        else:\n",
    "            buffered = BytesIO()\n",
    "            image.save(buffered, format=image_format)\n",
    "            img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "            return img_b64_str\n",
    "\n",
    "    def get_images(self, return_pil=False):\n",
    "        images = []\n",
    "        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n",
    "            if i % 2 == 0:\n",
    "                if type(msg) is tuple:\n",
    "                    msg, image, image_process_mode = msg\n",
    "                    image = self.process_image(image, image_process_mode, return_pil=return_pil)\n",
    "                    images.append(image)\n",
    "        return images\n",
    "\n",
    "    def to_gradio_chatbot(self):\n",
    "        ret = []\n",
    "        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n",
    "            if i % 2 == 0:\n",
    "                if type(msg) is tuple:\n",
    "                    msg, image, image_process_mode = msg\n",
    "                    img_b64_str = self.process_image(\n",
    "                        image, \"Default\", return_pil=False,\n",
    "                        image_format='JPEG')\n",
    "                    img_str = f'<img src=\"data:image/jpeg;base64,{img_b64_str}\" alt=\"user upload image\" />'\n",
    "                    msg = img_str + msg.replace('<image>', '').strip()\n",
    "                    ret.append([msg, None])\n",
    "                else:\n",
    "                    ret.append([msg, None])\n",
    "            else:\n",
    "                ret[-1][-1] = msg\n",
    "        return ret\n",
    "\n",
    "    def copy(self):\n",
    "        return Conversation(\n",
    "            system=self.system,\n",
    "            roles=self.roles,\n",
    "            messages=[[x, y] for x, y in self.messages],\n",
    "            offset=self.offset,\n",
    "            sep_style=self.sep_style,\n",
    "            sep=self.sep,\n",
    "            sep2=self.sep2,\n",
    "            version=self.version)\n",
    "\n",
    "    def dict(self):\n",
    "        if len(self.get_images()) > 0:\n",
    "            return {\n",
    "                \"system\": self.system,\n",
    "                \"roles\": self.roles,\n",
    "                \"messages\": [[x, y[0] if type(y) is tuple else y] for x, y in self.messages],\n",
    "                \"offset\": self.offset,\n",
    "                \"sep\": self.sep,\n",
    "                \"sep2\": self.sep2,\n",
    "            }\n",
    "        return {\n",
    "            \"system\": self.system,\n",
    "            \"roles\": self.roles,\n",
    "            \"messages\": self.messages,\n",
    "            \"offset\": self.offset,\n",
    "            \"sep\": self.sep,\n",
    "            \"sep2\": self.sep2,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_llava_v1 = Conversation(\n",
    "    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n",
    "           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n",
    "    roles=(\"USER\", \"ASSISTANT\"),\n",
    "    version=\"v1\",\n",
    "    messages=(),\n",
    "    offset=0,\n",
    "    sep_style=SeparatorStyle.TWO,\n",
    "    sep=\" \",\n",
    "    sep2=\"</s>\",\n",
    ")\n",
    "\n",
    "\n",
    "conv_templates = {\n",
    "    # \"default\": conv_vicuna_v0,\n",
    "    # \"v0\": conv_vicuna_v0,\n",
    "    # \"v1\": conv_vicuna_v1,\n",
    "    # \"vicuna_v1\": conv_vicuna_v1,\n",
    "    # \"llama_2\": conv_llama_2,\n",
    "    # \"mistral_instruct\": conv_mistral_instruct,\n",
    "    # \"chatml_direct\": conv_chatml_direct,\n",
    "    # \"mistral_direct\": conv_chatml_direct,\n",
    "\n",
    "    # \"plain\": conv_llava_plain,\n",
    "    # \"v0_plain\": conv_llava_plain,\n",
    "    # \"llava_v0\": conv_llava_v0,\n",
    "    # \"v0_mmtag\": conv_llava_v0_mmtag,\n",
    "    \"llava_v1\": conv_llava_v1,\n",
    "    # \"v1_mmtag\": conv_llava_v1_mmtag,\n",
    "    # \"llava_llama_2\": conv_llava_llama_2,\n",
    "\n",
    "    # \"mpt\": conv_mpt,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def disable_torch_init():\n",
    "    \"\"\"\n",
    "    Disable the redundant torch default initialization to accelerate model creation.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    setattr(torch.nn.Linear, \"reset_parameters\", lambda self: None)\n",
    "    setattr(torch.nn.LayerNorm, \"reset_parameters\", lambda self: None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def image_parser(args):\n",
    "    out = args.image_file.split(args.sep)\n",
    "    return out\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f369136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_anyres_image(image, processor, grid_pinpoints):\n",
    "    \"\"\"\n",
    "    Process an image with variable resolutions.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): The input image to be processed.\n",
    "        processor: The image processor object.\n",
    "        grid_pinpoints (str): A string representation of a list of possible resolutions.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the processed image patches.\n",
    "    \"\"\"\n",
    "    if type(grid_pinpoints) is list:\n",
    "        possible_resolutions = grid_pinpoints\n",
    "    else:\n",
    "        possible_resolutions = ast.literal_eval(grid_pinpoints)\n",
    "    best_resolution = select_best_resolution(image.size, possible_resolutions)\n",
    "    image_padded = resize_and_pad_image(image, best_resolution)\n",
    "\n",
    "    patches = divide_to_patches(image_padded, processor.crop_size['height'])\n",
    "\n",
    "    image_original_resize = image.resize((processor.size['shortest_edge'], processor.size['shortest_edge']))\n",
    "\n",
    "    image_patches = [image_original_resize] + patches\n",
    "    image_patches = [processor.preprocess(image_patch, return_tensors='pt')['pixel_values'][0]\n",
    "                     for image_patch in image_patches]\n",
    "    return torch.stack(image_patches, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_images(images, image_processor, model_cfg):\n",
    "    image_aspect_ratio = getattr(model_cfg, \"image_aspect_ratio\", None)\n",
    "    new_images = []\n",
    "    if image_aspect_ratio == 'pad':\n",
    "        for image in images:\n",
    "            image = expand2square(image, tuple(int(x*255) for x in image_processor.image_mean))\n",
    "            image = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "            new_images.append(image)\n",
    "    elif image_aspect_ratio == \"anyres\":\n",
    "        for image in images:\n",
    "            image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n",
    "            new_images.append(image)\n",
    "    else:\n",
    "        return image_processor(images, return_tensors='pt')['pixel_values']\n",
    "    if all(x.shape == new_images[0].shape for x in new_images):\n",
    "        new_images = torch.stack(new_images, dim=0)\n",
    "    return new_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eeca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split('<image>')]\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]\n",
    "\n",
    "    input_ids = []\n",
    "    offset = 0\n",
    "    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "        offset = 1\n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):\n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    if return_tensors is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(input_ids, dtype=torch.long)\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dabd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_model(args):\n",
    "    # Model\n",
    "    disable_torch_init()\n",
    "\n",
    "    model_name = get_model_name_from_path(args.model_path)\n",
    "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        args.model_path, args.model_base, model_name\n",
    "    )\n",
    "\n",
    "    qs = args.query\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    if IMAGE_PLACEHOLDER in qs:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
    "        else:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
    "    else:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            qs = image_token_se + \"\\n\" + qs\n",
    "        else:\n",
    "            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        conv_mode = \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        conv_mode = \"mpt\"\n",
    "    else:\n",
    "        conv_mode = \"llava_v0\"\n",
    "\n",
    "    if args.conv_mode is not None and conv_mode != args.conv_mode:\n",
    "        print(\n",
    "            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "                conv_mode, args.conv_mode, args.conv_mode\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        args.conv_mode = conv_mode\n",
    "\n",
    "    print(f\"Using conversation mode: {args.conv_mode}\")\n",
    "    conv = conv_templates[args.conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    image_files = image_parser(args)\n",
    "    images = load_images(image_files)\n",
    "    image_sizes = [x.size for x in images]\n",
    "    images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor,\n",
    "        model.config\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .cuda()\n",
    "    )\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=images_tensor,\n",
    "            image_sizes=image_sizes,\n",
    "            do_sample=True if args.temperature > 0 else False,\n",
    "            temperature=args.temperature,\n",
    "            top_p=args.top_p,\n",
    "            num_beams=args.num_beams,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "    print(outputs)\n",
    "\n",
    "prompt = \"What are the things I should be cautious about when I visit here?\"\n",
    "image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "\n",
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"model_base\": None,\n",
    "    \"model_name\": get_model_name_from_path(model_path),\n",
    "    \"query\": prompt,\n",
    "    \"conv_mode\": None,\n",
    "    \"image_file\": image_file,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 512\n",
    "})()\n",
    "\n",
    "eval_model(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
