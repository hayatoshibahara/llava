{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e869553",
   "metadata": {},
   "source": [
    "# LLaVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce932bb9",
   "metadata": {},
   "source": [
    "- [LLaVA: Large Language and Vision Assistant][1]\n",
    "- [haotian-liu/LLaVA][2]\n",
    "- [liuhaotian/LLaVA-Instruct-150K][3]\n",
    "\n",
    "[1]: https://llava-vl.github.io/\n",
    "[2]: https://github.com/haotian-liu/LLaVA\n",
    "[3]: https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070bfb5",
   "metadata": {},
   "source": [
    "## æ¦‚è¦\n",
    "\n",
    "LLaVAï¼ˆLarge Language and Vision Assistantï¼‰ã¯ã€å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆLMMï¼‰\n",
    "\n",
    "è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆVision Encoderï¼‰ã¨å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’çµ„ã¿åˆã‚ã›ãŸæ§‹é€ \n",
    "\n",
    "ãƒ†ã‚­ã‚¹ãƒˆç‰ˆã®GPT-4ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‹ã‚‰æˆã‚‹æŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚Šã€è¨“ç·´ï¼ˆVisual Instruction Tuningï¼‰\n",
    "\n",
    "æœªçŸ¥ã®ç”»åƒã‚„æŒ‡ç¤ºã«å¯¾ã—ã¦ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç‰ˆã®GPT-4ã«è¿‘ã„æ€§èƒ½ã‚’å®Ÿç¾ï¼ˆ85.1%ç›¸å¯¾ã‚¹ã‚³ã‚¢ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba274e15",
   "metadata": {},
   "source": [
    "## Visutal Instructionãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f50c91",
   "metadata": {},
   "source": [
    "åˆ©ç”¨å¯èƒ½ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æŒ‡ç¤ºè¿½å¾“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯å°‘ãªãã€ã‚¯ãƒ©ã‚¦ãƒ‰ã‚½ãƒ¼ã‚·ãƒ³ã‚°ã‚‚å±äººçš„ã§é›£ã—ã„\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ã®åé›†ã«GPT-4ã‚’æ´»ç”¨ã—ã€[COCOï¼ˆCommon Object in Contextï¼‰][1]ç”»åƒã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ:\n",
    "\n",
    "![](image/table1.png)\n",
    "\n",
    "[1]: https://cocodataset.org/#explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0eb00",
   "metadata": {},
   "source": [
    "- ä¸Šéƒ¨: ãƒ†ã‚­ã‚¹ãƒˆç‰ˆGPT-4ã«å…¥åŠ›ã™ã‚‹2ç¨®é¡ã®è¨˜å·è¡¨ç¾\n",
    "    1. ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³: è¦–è¦šçš„ãªã‚·ãƒ¼ãƒ³ã®æå†™\n",
    "    2. ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹: ã‚·ãƒ¼ãƒ³å†…ã®ç‰©ä½“ã®æ¦‚å¿µã¨ä½ç½®\n",
    "- ä¸‹éƒ¨: è¨˜å·è¡¨ç¾ã‹ã‚‰ç”Ÿæˆã—ãŸ3ç¨®é¡ã®æŒ‡ç¤ºè¿½å¾“ãƒ‡ãƒ¼ã‚¿ï¼ˆ158Kï¼‰\n",
    "    1. ä¼šè©±ï¼ˆ58Kï¼‰\n",
    "    2. è©³ç´°ãªè¨˜è¿°ï¼ˆ23Kï¼‰\n",
    "    3. è¤‡é›‘ãªæ¨è«–ï¼ˆ77Kï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee06133",
   "metadata": {},
   "source": [
    "## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05865cc2",
   "metadata": {},
   "source": [
    "äº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLanguage Modelï¼‰ã¨è¦–è¦šãƒ¢ãƒ‡ãƒ«ï¼ˆVision Encoderï¼‰ã‚’æ´»ç”¨:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90382e8c",
   "metadata": {},
   "source": [
    "![](image/fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c580d",
   "metadata": {},
   "source": [
    "- $X_v$: è¦–è¦šå…¥åŠ›\n",
    "- $Z_v$: è¦–è¦šç‰¹å¾´é‡\n",
    "- $H_v$: è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³\n",
    "- $X_q$: è¨€èªæŒ‡ç¤º\n",
    "- $H_q$: è¨€èªãƒˆãƒ¼ã‚¯ãƒ³\n",
    "- Language Model $f_\\phi$\n",
    "    - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ$\\phi$ã§ã‚ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«\n",
    "    - æŒ‡ç¤ºè¿½å¾“èƒ½åŠ›ã®é«˜ã„å…¬é–‹ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹[Vicuna][1]ã‚’ä½¿ç”¨\n",
    "- Vision Encoder $g$\n",
    "    - äº‹å‰å­¦ç¿’æ¸ˆã¿CLIPè¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ViT-L/14ã‚’ä½¿ç”¨\n",
    "    - ç”»åƒã‚’è¦–è¦šç‰¹å¾´é‡ã«å¤‰æ› $Z_v = g(X_v)$\n",
    "\n",
    "- $W$\n",
    "    - å°„å½±è¡Œåˆ—\n",
    "    - è¦–è¦šç‰¹å¾´é‡ã‚’è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ› $H_v = W\\cdot Z_v$\n",
    "    - è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨€èªãƒˆãƒ¼ã‚¯ãƒ³ã¨åŒæ§˜ã«æ‰±ã†\n",
    "\n",
    "[1]: https://lmsys.org/blog/2023-03-30-vicuna/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb40a94",
   "metadata": {},
   "source": [
    "## è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705925f",
   "metadata": {},
   "source": [
    "æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹æˆ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12dbdb",
   "metadata": {},
   "source": [
    "$$\n",
    "X_{\\text{instruct}}^{t} = \\begin{cases} \\text{Randomly choose } [X_{q}^{1}, X_{v}] \\text{ or } [X_{v}, X_{q}^{1}], & \\text{the first turn } t=1 \\\\ X_{q}^{t}, & \\text{the remaining turns } t>1 \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc6b1c",
   "metadata": {},
   "source": [
    "- æœ€åˆã®ã‚¿ãƒ¼ãƒ³ï¼ˆ$t=1$ï¼‰ã¯2ç¨®é¡ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«çµ„ã¿åˆã‚ã›ã‚‹\n",
    "    1. æœ€åˆã®è³ªå•$X_q^1$ã¨ç”»åƒ$X_v$\n",
    "    2. ç”»åƒ$X_v$ã¨æœ€åˆã®è³ªå•$X_q^1$\n",
    "- ãã‚Œä»¥é™ã®ã‚¿ãƒ¼ãƒ³ï¼ˆ$t \\gt 1$ï¼‰ã¯ã€è³ªå•$X_q^t$ã ã‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbf53e",
   "metadata": {},
   "source": [
    "æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å½¢å¼:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c87c36",
   "metadata": {},
   "source": [
    "![](image/table2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902db2c9",
   "metadata": {},
   "source": [
    "- ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸$X_{\\text{system-message}}$ã‚’è¿½åŠ ã—ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æ²¿ã‚ã›ã‚‹\n",
    "- è¨“ç·´æ™‚ã®æå¤±ã¯ã€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å›ç­”$X_a^t$ã¨çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³$<\\text{STOP}>$ã«å¯¾ã—ã¦è¨ˆç®—ã™ã‚‹ï¼ˆç·‘ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c55de5",
   "metadata": {},
   "source": [
    "è¨“ç·´ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ã„å›ç­”ã‚’ç”Ÿæˆã§ãã‚‹ç¢ºç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82712ad6",
   "metadata": {},
   "source": [
    "$$\n",
    "p(X_{a} \\mid X_{v}, X_{\\text{instruct}}) = \\prod_{i=1}^{L} p_{\\theta}(x_{i} \\mid X_{v}, X_{\\text{instruct}, <i}, X_{a, <i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312f8747",
   "metadata": {},
   "source": [
    "- $p(X_a | X_v, X_{\\text{instruct}})$\n",
    "    - ç”»åƒ$X_v$ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ$X_\\text{instruct}$ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®æ­£ã—ã„å›ç­”$X_a$ã‚’ç”Ÿæˆã§ãã‚‹ç¢ºç‡\n",
    "- $\\theta$: å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "- $X_v$: å…¥åŠ›ç”»åƒ\n",
    "- $X_{\\text{instruct}\\lt i}$: ç¾åœ¨ã®äºˆæ¸¬ãƒˆãƒ¼ã‚¯ãƒ³$X_i$ä»¥å‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "- $X_{a\\lt i}$: ç¾åœ¨ã®äºˆæ¸¬ãƒˆãƒ¼ã‚¯ãƒ³$X_i$ä»¥å‰ã®å›ç­”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19383595",
   "metadata": {},
   "source": [
    "Instruction Tuningã¯2æ®µéšã§è¡Œã†:\n",
    "\n",
    "1. ç‰¹å¾´é‡ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã®ãŸã‚ã®äº‹å‰å­¦ç¿’\n",
    "    - [CC3M][1]ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã€595Kã®ç”»åƒãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        - è‡ªç„¶è¨€èªãƒ©ã‚¤ãƒ–ãƒ©ãƒªSpacyã‚’ä½¿ã„å…¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰åè©å¥ã‚’æŠ½å‡ºã—ã€å‡ºç¾é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "        - é »åº¦ãŒ3æœªæº€ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "        - é »åº¦ãŒ100ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’é¸æŠã—åˆ¶é™\n",
    "    - ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ã€ã‚·ãƒ³ã‚°ãƒ«ã‚¿ãƒ¼ãƒ³ã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›\n",
    "        - GPT-4ãŒä½œæˆã—ãŸå˜ç´”ãªæŒ‡ç¤ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å…ƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿\n",
    "    - è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å‡çµã—ã€å°„å½±è¡Œåˆ—ã®ã¿ã‚’è¨“ç·´ï¼ˆ$\\theta = W$ï¼‰\n",
    "    - ç”»åƒç‰¹å¾´é‡$H_v$ã¨è¨€èªç‰¹å¾´é‡$H_q$ã®ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã‚’ã¨ã‚‹\n",
    "2. ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "    - å°„å½±è¡Œåˆ—ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã®ä¸¡æ–¹ã‚’è¨“ç·´ï¼ˆ$\\theta = {W, \\phi}$ï¼‰\n",
    "    - 2ç¨®é¡ã®æ‰‹æ³•ã§å®Ÿé¨“\n",
    "        1. ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆç”¨\n",
    "            - COCOã§ä½œæˆã—ãŸ158Kã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨\n",
    "            - ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ã€ã‚·ãƒ³ã‚°ãƒ«ã‚¿ãƒ¼ãƒ³ã®è©³ç´°ãªè¨˜è¿°ãƒ»è¤‡é›‘ãªæ¨è«–\n",
    "            - 3ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡ä¸€ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "        2. ç§‘å­¦ã®ã‚¯ã‚¤ã‚ºç”¨\n",
    "            - [ScienceQA](https://scienceqa.github.io)ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨\n",
    "            - è³ªå•ã«å¯¾ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸ãˆã€è¤‡æ•°ã®é¸æŠè‚¢ã‹ã‚‰å›ç­”ã‚’é¸æŠã•ã›ã‚‹\n",
    "            - ã‚·ãƒ³ã‚°ãƒ«ã‚¿ãƒ¼ãƒ³ã®ä¼šè©±å½¢å¼\n",
    "\n",
    "[1]: https://huggingface.co/datasets/pixparse/cc3m-wds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960737fa",
   "metadata": {},
   "source": [
    "## å®Ÿé¨“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b943b",
   "metadata": {},
   "source": [
    "Vicunaã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¾“ã„ã€8æšã®A100 GPUã§è¨“ç·´\n",
    "\n",
    "äº‹å‰å­¦ç¿’è¨­å®š:\n",
    "\n",
    "- CC-595Kãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨\n",
    "- å­¦ç¿’ç‡ 2e-3\n",
    "- ãƒãƒƒãƒã‚µã‚¤ã‚º 128\n",
    "- 1ã‚¨ãƒãƒƒã‚¯\n",
    "\n",
    "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°:\n",
    "\n",
    "- LLaVA-Instruct-158Kãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "- å­¦ç¿’ç‡2e-5\n",
    "- ãƒãƒƒãƒã‚µã‚¤ã‚º32\n",
    "- 3ã‚¨ãƒãƒƒã‚¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9755a",
   "metadata": {},
   "source": [
    "LLaVAã¯ã€å˜ãªã‚‹ã‚·ãƒ¼ãƒ³ã®èª¬æ˜ã§ã¯ãªãæŒ‡ç¤ºã«æ­£ç¢ºã«å¾“ã£ãŸå›ç­”ãŒå¯èƒ½:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b8aa5",
   "metadata": {},
   "source": [
    "![](image/table3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4087fb",
   "metadata": {},
   "source": [
    "GPT-4ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã®å“è³ªã‚’å®šé‡çš„ã«è©•ä¾¡ã—ã€è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã«æ¯”è¼ƒ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d57e5",
   "metadata": {},
   "source": [
    "![](image/table4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cbfb2e",
   "metadata": {},
   "source": [
    "- äº‹å‰å­¦ç¿’ï¼ˆInstruction Tuningï¼‰ã§ã€æŒ‡ç¤ºè¿½å¾“èƒ½åŠ›ãŒ50ç‚¹ä»¥ä¸Šæ”¹å–„ï¼ˆ73.8ï¼‰\n",
    "- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã€æ¨è«–èƒ½åŠ›ãŒæ”¹å–„ã—æ›´ã«7ç‚¹æ”¹å–„ï¼ˆ85.1ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16cf635",
   "metadata": {},
   "source": [
    "OpenFlamingoã‚„BLIP-2ã¨æ¯”ã¹ã‚‹ã¨è‘—ã—ãé«˜ã„æ€§èƒ½:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785081cf",
   "metadata": {},
   "source": [
    "![](image/table5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d56094",
   "metadata": {},
   "source": [
    "LLaVAãŒè‹¦æ‰‹ãªè³ªå•:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e4dae",
   "metadata": {},
   "source": [
    "![](image/table6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e04442",
   "metadata": {},
   "source": [
    "- å†™çœŸã‹ã‚‰ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã®åå‰ã‚’å½“ã¦ã‚‹\n",
    "- å†™çœŸã‹ã‚‰ãƒ¨ãƒ¼ã‚°ãƒ«ãƒˆã®ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’å½“ã¦ã‚‹\n",
    "- ãƒ¨ãƒ¼ã‚°ãƒ«ãƒˆã¨ã‚¤ãƒã‚´ãŒåˆ¥ã«ã‚ã‚‹ã®ã«ã‚¤ãƒã‚´å‘³ã®ãƒ¨ãƒ¼ã‚°ãƒ«ãƒˆãŒã‚ã‚‹ã¨ç­”ãˆã¦ã—ã¾ã†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c3d72",
   "metadata": {},
   "source": [
    "ç§‘å­¦ã®å•é¡Œã«ã¤ã„ã¦ã¯ã€SOTAã«è¿‘ã„æ€§èƒ½ï¼ˆ90.92%ï¼‰:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c02c23",
   "metadata": {},
   "source": [
    "![](image/table7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5500e1",
   "metadata": {},
   "source": [
    "- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ 12726ä»¶ã€æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿4,241ä»¶ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ 4241ä»¶\n",
    "- 12ã‚¨ãƒãƒƒã‚¯ã§è¨“ç·´\n",
    "- ãƒ¢ãƒ‡ãƒ«ã«ç†ç”±ã‚’äºˆæ¸¬ã•ã›ã¦ã€å›ç­”ã•ã›ã‚‹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967a89a",
   "metadata": {},
   "source": [
    "## LLaVA 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af3c6c",
   "metadata": {},
   "source": [
    "LLaVA 1.5ã¯ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®LLaVAã«å°‘ã—ä¿®æ­£ã‚’åŠ ãˆã¦æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ãŸãƒ¢ãƒ‡ãƒ«:\n",
    "\n",
    "1. è¦–è¦šã¨è¨€èªã‚’ã¤ãªãã‚³ãƒã‚¯ã‚¿ï¼ˆVision-Language Connectorï¼‰ã‚’å¤‰æ›´\n",
    "    - 1ã¤ã®å…¨çµåˆå±¤ã‹ã‚‰ã€2å±¤ã®MLPï¼ˆå…¨çµåˆå±¤->GELU->å…¨çµåˆå±¤ï¼‰\n",
    "2. å…¥åŠ›ç”»åƒè§£åƒåº¦ã‚’æ‹¡å¤§ï¼ˆ224x224ã‹ã‚‰336x336ï¼‰\n",
    "  - ãƒ‘ãƒƒãƒã¯14pxå››æ–¹ãªã®ã§ã€14*14=576ã«+1ï¼ˆCLSãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ãŒç”»åƒç‰¹å¾´é‡ã®é•·ã•\n",
    "3. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å¢—å¼·ï¼ˆå­¦è¡“ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼‰\n",
    "    - äº‹å‰å­¦ç¿’ï¼ˆ8å°A100ã§13Bã‚’5.5æ™‚é–“ã€7Bã‚’3.5æ™‚é–“ï¼‰\n",
    "        - [CC-3M Cocept-balanced 595K](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K)\n",
    "            - [CC-3M](https://github.com/google-research-datasets/conceptual-captions)ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã—ã€BLIPã§ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æ§‹æˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æ§‹æˆ\n",
    "        - [LAION/CC/SBU BLIP-Caption Concept-balanced 558K](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain)\n",
    "          - LAION-CC-SBUã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§BLIPã‚’ä½¿ã£ã¦ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’é«˜å“è³ªåŒ–\n",
    "    - ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆ8å°A100ã§13Bã‚’20æ™‚é–“ã€7Bã§10æ™‚é–“ã€LORAã§ãƒ¡ãƒ¢ãƒªå‰Šæ¸›å¯èƒ½ï¼‰\n",
    "        - [LLaVA-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)\n",
    "            - GPT-4ã‚’ç”¨ã„ã¦ç”Ÿæˆã—ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªæŒ‡ç¤ºè¿½å¾“ãƒ‡ãƒ¼ã‚¿ï¼ˆ[ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ](https://github.com/haotian-liu/LLaVA/tree/main/playground/data/prompts)ï¼‰\n",
    "        - [COCO train2017](http://images.cocodataset.org/zips/train2017.zip)\n",
    "            - ç‰©ä½“æ¤œå‡ºç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "        - [GQA](https://cs.stanford.edu/people/dorarad/gqa/)\n",
    "            - [ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip)\n",
    "            - æ§‹æˆçš„ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "        - [OCR-VQA](https://huggingface.co/datasets/howard-hou/OCR-VQA)\n",
    "            - [ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰](https://drive.google.com/drive/folders/1_GYPY5UkUy7HIcR0zq3ZCFgeZN7BAfm_?usp=sharing)\n",
    "            - ç”»åƒä¸­ã®æ–‡å­—ã®è³ªå•å¿œç­”\n",
    "        - TextVQA\n",
    "            - [JSON](https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5.1_val.json)\n",
    "            - [ç”»åƒ](https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip)\n",
    "            - ç”»åƒä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆã®èª­ã¿å–ã‚Šã¨æ¨è«–\n",
    "        - [VisualGenome](https://arxiv.org/abs/1602.07332)\n",
    "            - ç”»åƒã®è©³ç´°ãªé ˜åŸŸèª¬æ˜\n",
    "            - [part1](https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip)\n",
    "            - [part2](https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip)\n",
    "4. å‡ºåŠ›å½¢å¼ã‚’æŒ‡å®šã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å°å…¥ï¼ˆçŸ­ã„å›ç­”ã¨è‡ªç„¶ãªå›ç­”ã‚’ä½¿ã„åˆ†ã‘ã‚‹ã“ã¨ãŒå¯èƒ½ã«ï¼‰\n",
    "    - \"Answer the question using single word or phrase.\"ãªã©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba63137",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3a872",
   "metadata": {},
   "source": [
    "- [llava@1.2.2.post1][1]\n",
    "\n",
    "[1]: https://github.com/haotian-liu/LLaVA/releases/tag/v1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "if os.path.exists(\"debug.log\"):\n",
    "    os.remove(\"debug.log\")\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ğŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ğŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ğŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ğŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ğŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(\"debug.log\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "NVIDIA_SMI = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout\n",
    "logging.info(NVIDIA_SMI)\n",
    "logging.info(f\"Python {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q bitsandbytes sentencepiece protobuf\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import auto, Enum\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig, LlamaConfig, LlamaModel, LlamaForCausalLM\n",
    "from transformers.generation.utils import GenerateOutput\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import base64\n",
    "import dataclasses\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import warnings\n",
    "\n",
    "logger.info(f\"Transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7f42d",
   "metadata": {},
   "source": [
    "### Vision Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aca916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPVisionTower(nn.Module):\n",
    "    \"\"\"\n",
    "    ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€CLIPã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹\n",
    "    build_vision_toweré–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "    LlavaMetaModelã‚¯ãƒ©ã‚¹ã§å®Ÿä½“åŒ–ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vision_tower, args, delay_load=False):\n",
    "        logger.info(f\"CLIPVisionTowerã‚’åˆæœŸåŒ– {vision_tower=} {args=} {delay_load=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_loaded = False\n",
    "\n",
    "        # openai/clip-vit-large-patch14-336\n",
    "        # https://huggingface.co/openai/clip-vit-large-patch14-336\n",
    "        self.vision_tower_name = vision_tower\n",
    "\n",
    "        # CLIPã®éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰æœ€å¾Œã‹ã‚‰Nç•ªç›®ã®å±¤ã‚’ä½¿ç”¨ã™ã‚‹\n",
    "        # -2ï¼ˆæœ€å¾Œã‹ã‚‰2ç•ªç›®ã®å±¤ï¼‰\n",
    "        self.select_layer = args.mm_vision_select_layer\n",
    "\n",
    "        # éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰é¸æŠã™ã‚‹ç‰¹å¾´é‡ã®ç¨®é¡\n",
    "        # patchï¼ˆãƒ‘ãƒƒãƒç‰¹å¾´é‡ã®ã¿ï¼‰\n",
    "        self.select_feature = getattr(args, 'mm_vision_select_feature', 'patch')\n",
    "\n",
    "        # False\n",
    "        if not delay_load:\n",
    "            self.load_model()\n",
    "\n",
    "        # False\n",
    "        elif getattr(args, 'unfreeze_mm_vision_tower', False):\n",
    "            self.load_model()\n",
    "\n",
    "        # True\n",
    "        else:\n",
    "            # CLIPVisionConfigã‚’èª­ã¿è¾¼ã‚€\n",
    "            # https://huggingface.co/openai/clip-vit-large-patch14-336/blob/main/config.json\n",
    "            self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)\n",
    "            logger.debug(f\"CLIPVisionConfigã‚’èª­ã¿è¾¼ã‚€ {self.cfg_only=}\")\n",
    "\n",
    "    def load_model(self, device_map=None):\n",
    "        \"\"\"\n",
    "        CLIPImageProcessorã¨CLIPVisionModelã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "        load_pretrained_modelé–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "        \"\"\"\n",
    "        logger.info(f\"CLIPVisionTowerã‚’èª­ã¿è¾¼ã¿ {device_map=}\")\n",
    "\n",
    "        # 1) å…¥åŠ›å€¤ã®æ¤œè¨¼\n",
    "\n",
    "        if self.is_loaded:\n",
    "            print('{} is already loaded, `load_model` called again, skipping.'.format(self.vision_tower_name))\n",
    "            return\n",
    "\n",
    "        # 2) CLIPImageProcessorã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "        self.image_processor = CLIPImageProcessor.from_pretrained(\n",
    "            self.vision_tower_name\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"CLIPImageProcessorã‚’ãƒ­ãƒ¼ãƒ‰ {self.image_processor=}\")\n",
    "\n",
    "        # 3) CLIPVisionModelã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "        self.vision_tower = CLIPVisionModel.from_pretrained(\n",
    "            self.vision_tower_name,\n",
    "            device_map=device_map\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"CLIPVisionModelã‚’ãƒ­ãƒ¼ãƒ‰ {self.vision_tower=}\")\n",
    "\n",
    "        # 4) å¾Œå‡¦ç†\n",
    "\n",
    "        # ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å›ºå®š\n",
    "        self.vision_tower.requires_grad_(False)\n",
    "\n",
    "        # èª­ã¿è¾¼ã¿æ¸ˆã¿ãƒ•ãƒ©ã‚°ã‚’æœ‰åŠ¹åŒ–\n",
    "        self.is_loaded = True\n",
    "\n",
    "    def feature_select(self, image_forward_outs):\n",
    "        \"\"\"\n",
    "        é †ä¼æ’­ç›´å¾Œã®éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰ç‰¹å¾´é‡ã‚’é¸æŠã™ã‚‹\n",
    "        forwardé–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            image_forward_outs: CLIPVisionModelã®é †ä¼æ¬å‡ºåŠ›\n",
    "        Returns:\n",
    "            image_features: é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡\n",
    "        \"\"\"\n",
    "        logger.info(f\"CLIPVisionTowerã®ç‰¹å¾´é‡é¸æŠ {type(image_forward_outs)=}\")\n",
    "\n",
    "        image_features = image_forward_outs.hidden_states[self.select_layer]\n",
    "        logger.debug(f\"é¸æŠã•ã‚ŒãŸå±¤ã®éš ã‚ŒçŠ¶æ…‹ {image_features.shape=}\")\n",
    "\n",
    "        # True\n",
    "        if self.select_feature == 'patch':\n",
    "            # 0ç•ªç›®ã¯CLSãƒˆãƒ¼ã‚¯ãƒ³ãªã®ã§é™¤å¤–\n",
    "            image_features = image_features[:, 1:]\n",
    "            logger.debug(f\"ãƒ‘ãƒƒãƒç‰¹å¾´é‡ã®ã¿é¸æŠ {image_features.shape=}\")\n",
    "\n",
    "        # False\n",
    "        elif self.select_feature == 'cls_patch':\n",
    "            image_features = image_features\n",
    "\n",
    "        # False\n",
    "        else:\n",
    "            raise ValueError(f'Unexpected select feature: {self.select_feature}')\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, images):\n",
    "        logger.info(f\"CLIPVisionTowerã®é †ä¼æ¬ {type(images)=}\")\n",
    "\n",
    "        # ç”»åƒãŒãƒªã‚¹ãƒˆã®å ´åˆ\n",
    "        # False\n",
    "        if type(images) is list:\n",
    "            image_features = []\n",
    "            for image in images:\n",
    "                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)\n",
    "                image_feature = self.feature_select(image_forward_out).to(image.dtype)\n",
    "                image_features.append(image_feature)\n",
    "\n",
    "        # ç”»åƒãŒ1æšã®å ´åˆ\n",
    "        else:\n",
    "\n",
    "            # GPUã«è»¢é€ã—ã¦é †ä¼æ¬ã—ã€éš ã‚ŒçŠ¶æ…‹ã‚’å‡ºåŠ›\n",
    "            image_forward_outs = self.vision_tower(\n",
    "                images.to(device=self.device, dtype=self.dtype),\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "\n",
    "            # éš ã‚ŒçŠ¶æ…‹ã‹ã‚‰ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "            image_features = self.feature_select(image_forward_outs).to(images.dtype)\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    @property\n",
    "    def dummy_feature(self):\n",
    "        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.vision_tower.dtype\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.vision_tower.device\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        if self.is_loaded:\n",
    "            return self.vision_tower.config\n",
    "        else:\n",
    "            return self.cfg_only\n",
    "\n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        return self.config.hidden_size\n",
    "\n",
    "    @property\n",
    "    def num_patches_per_side(self):\n",
    "        return self.config.image_size // self.config.patch_size\n",
    "\n",
    "    @property\n",
    "    def num_patches(self):\n",
    "        return (self.config.image_size // self.config.patch_size) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b9e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vision_tower(vision_tower_cfg, **kwargs):\n",
    "    \"\"\"\n",
    "    ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã‚’å®Ÿä½“åŒ–ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°\n",
    "    LlavaMetaModelã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "    Args:\n",
    "        vision_tower_cfg: ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã®è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        **kwargs: ãã®ä»–ã®å¼•æ•°\n",
    "    Returns:\n",
    "        æ§‹ç¯‰ã•ã‚ŒãŸãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "    \"\"\"\n",
    "    logger.info(f\"ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã‚’æ§‹ç¯‰ {vision_tower_cfg=}, {kwargs=}\")\n",
    "\n",
    "    # 1) ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
    "\n",
    "    # ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã®åå‰ã‚’å–å¾—\n",
    "    # openai/clip-vit-large-patch14-336\n",
    "    # https://huggingface.co/liuhaotian/llava-v1.5-7b/blob/main/config.json\n",
    "    vision_tower = getattr(\n",
    "        vision_tower_cfg,\n",
    "        'mm_vision_tower',\n",
    "        getattr(vision_tower_cfg, 'vision_tower', None)\n",
    "    )\n",
    "\n",
    "    is_absolute_path_exists = os.path.exists(vision_tower)\n",
    "    logger.debug(f\"{is_absolute_path_exists=}\")\n",
    "\n",
    "    # ClipVisionTowerS2ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼ˆæœ€æ–°ç‰ˆã®ClipVisionTowerï¼‰\n",
    "    # False\n",
    "    use_s2 = getattr(vision_tower_cfg, 's2', False)\n",
    "    logger.debug(f\"{use_s2=}\")\n",
    "\n",
    "    # 2) ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã®å®Ÿä½“åŒ–\n",
    "\n",
    "    # True\n",
    "    if is_absolute_path_exists or \\\n",
    "        vision_tower.startswith(\"openai\") or \\\n",
    "        vision_tower.startswith(\"laion\") or \\\n",
    "        \"ShareGPT4V\" in vision_tower:\n",
    "\n",
    "        # False\n",
    "        if use_s2:\n",
    "            return CLIPVisionTowerS2(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "        else:\n",
    "            # CLIPVisionTowerã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
    "            return CLIPVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)\n",
    "\n",
    "    raise ValueError(f'Unknown vision tower: {vision_tower}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8b3e2",
   "metadata": {},
   "source": [
    "### Vision Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vision_projector(config, delay_load=False, **kwargs):\n",
    "    \"\"\"\n",
    "    ãƒ“ã‚¸ãƒ§ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã‚’æ§‹ç¯‰ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°\n",
    "    LlavaMetaModelã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "    ä»Šå›ã¯GELUæ´»æ€§åŒ–é–¢æ•°ã‚’æ­è¼‰ã—ãŸ2å±¤ã®å…¨çµåˆå±¤ã‹ã‚‰ãªã‚‹MLPã‚’ä½¿ç”¨\n",
    "    ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã®å‡ºåŠ›ç‰¹å¾´é‡ã‚’è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        config: ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        delay_load: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã®é…å»¶ãƒ­ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°\n",
    "        **kwargs: ãã®ä»–ã®å¼•æ•°\n",
    "    Returns:\n",
    "        æ§‹ç¯‰ã•ã‚ŒãŸãƒ“ã‚¸ãƒ§ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "    \"\"\"\n",
    "    logger.info(f\"ãƒ“ã‚¸ãƒ§ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã‚’æ§‹ç¯‰ {config=}, {delay_load=}, {kwargs=}\")\n",
    "\n",
    "    # 1) ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º\n",
    "\n",
    "    # https://huggingface.co/liuhaotian/llava-v1.5-7b/blob/main/config.json\n",
    "    # mlp2x_gelu: GELUä»˜ãã®2å±¤MLP\n",
    "    projector_type = getattr(config, 'mm_projector_type', 'linear')\n",
    "    logger.debug(f\"{projector_type=}\")\n",
    "\n",
    "    # False\n",
    "    if projector_type == 'linear':\n",
    "        print(\"Using linear projector for vision features.\")\n",
    "        return nn.Linear(config.mm_hidden_size, config.hidden_size)\n",
    "\n",
    "    # mlp2x_gelu -> 2ï¼ˆ2å±¤ã®å…¨çµåˆå±¤ã‹ã‚‰ãªã‚‹MLPï¼‰\n",
    "    mlp_gelu_match = re.match(r'^mlp(\\d+)x_gelu$', projector_type)\n",
    "\n",
    "    # True\n",
    "    if mlp_gelu_match:\n",
    "\n",
    "        # 2\n",
    "        mlp_depth = int(mlp_gelu_match.group(1))\n",
    "        logger.debug(f\"{mlp_depth=}\")\n",
    "\n",
    "        modules = [\n",
    "            # 1ã¤ç›®ã®å…¨çµåˆå±¤\n",
    "            # 1024 -> 4096\n",
    "            nn.Linear(config.mm_hidden_size, config.hidden_size)\n",
    "        ]\n",
    "\n",
    "        # 2å±¤ãªã®ã§1å›ãƒ«ãƒ¼ãƒ—\n",
    "        for _ in range(1, mlp_depth):\n",
    "\n",
    "            # GELUæ´»æ€§åŒ–é–¢æ•°\n",
    "            modules.append(nn.GELU())\n",
    "\n",
    "            # 2ã¤ç›®ã®å…¨çµåˆå±¤\n",
    "            # 4096 -> 4096\n",
    "            modules.append(nn.Linear(config.hidden_size, config.hidden_size))\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    if projector_type == 'identity':\n",
    "        return IdentityMap()\n",
    "\n",
    "    raise ValueError(f'Unknown projector type: {projector_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd227f",
   "metadata": {},
   "source": [
    "### LlavaMetaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaMetaModel:\n",
    "    \"\"\"\n",
    "    ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã¨ãƒ“ã‚¸ãƒ§ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã‚’ãƒ©ãƒƒãƒ—ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    ç”»åƒã®å…¥åŠ›ã‚’ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã§ç”»åƒç‰¹å¾´é‡ã«å¤‰æ›ã—ã€ãƒ“ã‚¸ãƒ§ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã§è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã™ã‚‹\n",
    "    LlavaLlamaModelãŒç¶™æ‰¿ã—ã€è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLlamaModelï¼‰ã¨çµ±åˆã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(f\"LlavaMetaModelã‚’åˆæœŸåŒ– {config=}\")\n",
    "\n",
    "        # https://huggingface.co/liuhaotian/llava-v1.5-7b/blob/main/config.json\n",
    "        super(LlavaMetaModel, self).__init__(config)\n",
    "\n",
    "        # True\n",
    "        if hasattr(config, \"mm_vision_tower\"):\n",
    "\n",
    "            # ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã®æ§‹ç¯‰\n",
    "            self.vision_tower = build_vision_tower(config, delay_load=True)\n",
    "\n",
    "            # ãƒ“ã‚¸ãƒ§ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã®æ§‹ç¯‰\n",
    "            self.mm_projector = build_vision_projector(config)\n",
    "\n",
    "            # False\n",
    "            if 'unpad' in getattr(config, 'mm_patch_merge_type', ''):\n",
    "                self.image_newline = nn.Parameter(\n",
    "                    torch.empty(config.hidden_size, dtype=self.dtype)\n",
    "                )\n",
    "\n",
    "    def get_vision_tower(self):\n",
    "        \"\"\"\n",
    "        ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã‚’å–å¾—ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°\n",
    "        åˆæœŸåŒ–ã‚„ç”»åƒç‰¹å¾´é‡æŠ½å‡ºæ™‚ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "\n",
    "        Returns:\n",
    "            ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "        \"\"\"\n",
    "        vision_tower = getattr(self, 'vision_tower', None)\n",
    "        if type(vision_tower) is list:\n",
    "            vision_tower = vision_tower[0]\n",
    "        return vision_tower\n",
    "\n",
    "    def initialize_vision_modules(self, model_args, fsdp=None):\n",
    "        \"\"\"\n",
    "        ä½¿ã‚ãªã„\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åˆæœŸåŒ– {model_args=}, {fsdp=}\")\n",
    "        vision_tower = model_args.vision_tower\n",
    "        mm_vision_select_layer = model_args.mm_vision_select_layer\n",
    "        mm_vision_select_feature = model_args.mm_vision_select_feature\n",
    "        pretrain_mm_mlp_adapter = model_args.pretrain_mm_mlp_adapter\n",
    "        mm_patch_merge_type = model_args.mm_patch_merge_type\n",
    "\n",
    "        self.config.mm_vision_tower = vision_tower\n",
    "\n",
    "        if self.get_vision_tower() is None:\n",
    "            vision_tower = build_vision_tower(model_args)\n",
    "\n",
    "            if fsdp is not None and len(fsdp) > 0:\n",
    "                self.vision_tower = [vision_tower]\n",
    "            else:\n",
    "                self.vision_tower = vision_tower\n",
    "        else:\n",
    "            if fsdp is not None and len(fsdp) > 0:\n",
    "                vision_tower = self.vision_tower[0]\n",
    "            else:\n",
    "                vision_tower = self.vision_tower\n",
    "            vision_tower.load_model()\n",
    "\n",
    "        self.config.use_mm_proj = True\n",
    "        self.config.mm_projector_type = getattr(model_args, 'mm_projector_type', 'linear')\n",
    "        self.config.mm_hidden_size = vision_tower.hidden_size\n",
    "        self.config.mm_vision_select_layer = mm_vision_select_layer\n",
    "        self.config.mm_vision_select_feature = mm_vision_select_feature\n",
    "        self.config.mm_patch_merge_type = mm_patch_merge_type\n",
    "\n",
    "        if getattr(self, 'mm_projector', None) is None:\n",
    "            self.mm_projector = build_vision_projector(self.config)\n",
    "\n",
    "            if 'unpad' in mm_patch_merge_type:\n",
    "                embed_std = 1 / torch.sqrt(torch.tensor(self.config.hidden_size, dtype=self.dtype))\n",
    "                self.image_newline = nn.Parameter(\n",
    "                    torch.randn(self.config.hidden_size, dtype=self.dtype) * embed_std\n",
    "                )\n",
    "        else:\n",
    "            # In case it is frozen by LoRA\n",
    "            for p in self.mm_projector.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        if pretrain_mm_mlp_adapter is not None:\n",
    "            mm_projector_weights = torch.load(pretrain_mm_mlp_adapter, map_location='cpu')\n",
    "            def get_w(weights, keyword):\n",
    "                return {k.split(keyword + '.')[1]: v for k, v in weights.items() if keyword in k}\n",
    "\n",
    "            self.mm_projector.load_state_dict(get_w(mm_projector_weights, 'mm_projector'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efab233",
   "metadata": {},
   "source": [
    "### LlavaMetaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaMetaForCausalLM(ABC):\n",
    "    \"\"\"\n",
    "    ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã®æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹\n",
    "    LlavaLlamaForCausalLMãŒç¶™æ‰¿ã—ã€LlavaMetaModelã¨LlamaForCausalLMã‚’çµ±åˆã™ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_model(self):\n",
    "        pass\n",
    "\n",
    "    def get_vision_tower(self):\n",
    "        \"\"\"\n",
    "        ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã‚’å–å¾—ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°\n",
    "        prepare_inputs_labels_for_multimodalé–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "        \"\"\"\n",
    "        return self.get_model().get_vision_tower()\n",
    "\n",
    "    def encode_images(self, images):\n",
    "        \"\"\"\n",
    "        ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹\n",
    "        prepare_inputs_labels_for_multimodalé–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            images: ç”»åƒãƒ‡ãƒ¼ã‚¿ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            image_features: æŠ½å‡ºã•ã‚ŒãŸç”»åƒç‰¹å¾´é‡ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ {images.shape=}\")\n",
    "\n",
    "        # ç”»åƒã‚’ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ç”»åƒç‰¹å¾´é‡ã‚’å–å¾—\n",
    "        image_features = self.get_model().get_vision_tower()(images)\n",
    "        logger.debug(f\"ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ {image_features.shape=}\")\n",
    "\n",
    "        # ç”»åƒç‰¹å¾´é‡ã‚’ãƒ“ã‚¸ãƒ§ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã§è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›\n",
    "        image_features = self.get_model().mm_projector(image_features)\n",
    "        logger.debug(f\"ãƒ“ã‚¸ãƒ§ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã§å¤‰æ› {image_features.shape=}\")\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    def prepare_inputs_labels_for_multimodal(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes=None):\n",
    "        \"\"\"\n",
    "        ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã«å¿…è¦ãªãƒ†ãƒ³ã‚½ãƒ«ã‚’æº–å‚™ã™ã‚‹\n",
    "        è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®<image>ã®ä½ç½®ã«æŒ¿å…¥ã—ã€å¯¾å¿œã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã¨ãƒ©ãƒ™ãƒ«ã‚’æ›´æ–°ã™ã‚‹\n",
    "        LlavaLlamaForCausalLMã®é †ä¼æ’­ã®æœ€åˆï¼ˆPrefillï¼‰ã¨ç”Ÿæˆï¼ˆDecodeï¼‰ã®åº¦ã«å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            input_ids: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            position_ids: ä½ç½®IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            attention_mask: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            past_key_values: éå»ã®ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            labels: ãƒ©ãƒ™ãƒ«ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            images: ç”»åƒãƒ‡ãƒ¼ã‚¿ã®ãƒ†ãƒ³ã‚½ãƒ«ã¾ãŸã¯ãƒªã‚¹ãƒˆ\n",
    "            image_sizes: ç”»åƒã‚µã‚¤ã‚ºã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "        Returns:\n",
    "            input_ids: æ›´æ–°ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            position_ids: æ›´æ–°ã•ã‚ŒãŸä½ç½®IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            attention_mask: æ›´æ–°ã•ã‚ŒãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            past_key_values: æ›´æ–°ã•ã‚ŒãŸéå»ã®ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            image_features: æŠ½å‡ºã•ã‚ŒãŸç”»åƒç‰¹å¾´é‡ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            labels: æ›´æ–°ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã®æº–å‚™ {input_ids.shape=}, {position_ids.shape if position_ids is not None else None=}, {attention_mask.shape if attention_mask is not None else None=}, {past_key_values=}, {labels.shape if labels is not None else None=}, {type(images)=}, {image_sizes=}\")\n",
    "\n",
    "        # 1) ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "\n",
    "        # ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã‚’å–å¾—\n",
    "        vision_tower = self.get_vision_tower()\n",
    "\n",
    "        # ç”»åƒãŒãªã„å ´åˆ\n",
    "        # False\n",
    "        if vision_tower is None or images is None or input_ids.shape[1] == 1:\n",
    "            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n",
    "\n",
    "        # ç”»åƒãŒè¤‡æ•°ã‚ã‚‹å ´åˆ\n",
    "        # False\n",
    "        if type(images) is list or images.ndim == 5:\n",
    "\n",
    "            if type(images) is list:\n",
    "                images = [x.unsqueeze(0) if x.ndim == 3 else x for x in images]\n",
    "\n",
    "            concat_images = torch.cat([image for image in images], dim=0)\n",
    "\n",
    "            image_features = self.encode_images(concat_images)\n",
    "\n",
    "            split_sizes = [image.shape[0] for image in images]\n",
    "\n",
    "            image_features = torch.split(image_features, split_sizes, dim=0)\n",
    "\n",
    "            mm_patch_merge_type = getattr(\n",
    "                self.config,\n",
    "                'mm_patch_merge_type',\n",
    "                'flat'\n",
    "            )\n",
    "\n",
    "            image_aspect_ratio = getattr(\n",
    "                self.config,\n",
    "                'image_aspect_ratio',\n",
    "                'square'\n",
    "            )\n",
    "\n",
    "            if mm_patch_merge_type == 'flat':\n",
    "                image_features = [x.flatten(0, 1) for x in image_features]\n",
    "\n",
    "            elif mm_patch_merge_type.startswith('spatial'):\n",
    "                new_image_features = []\n",
    "                for image_idx, image_feature in enumerate(image_features):\n",
    "                    if image_feature.shape[0] > 1:\n",
    "                        base_image_feature = image_feature[0]\n",
    "                        image_feature = image_feature[1:]\n",
    "                        height = width = self.get_vision_tower().num_patches_per_side\n",
    "                        assert height * width == base_image_feature.shape[0]\n",
    "                        if image_aspect_ratio == 'anyres':\n",
    "                            num_patch_width, num_patch_height = get_anyres_image_grid_shape(image_sizes[image_idx], self.config.image_grid_pinpoints, self.get_vision_tower().config.image_size)\n",
    "                            image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n",
    "                        else:\n",
    "                            raise NotImplementedError\n",
    "                        if 'unpad' in mm_patch_merge_type:\n",
    "                            image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n",
    "                            image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n",
    "                            image_feature = unpad_image(image_feature, image_sizes[image_idx])\n",
    "                            image_feature = torch.cat((\n",
    "                                image_feature,\n",
    "                                self.model.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1).to(image_feature.device)\n",
    "                            ), dim=-1)\n",
    "                            image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n",
    "                        else:\n",
    "                            image_feature = image_feature.permute(0, 2, 1, 3, 4).contiguous()\n",
    "                            image_feature = image_feature.flatten(0, 3)\n",
    "                        image_feature = torch.cat((base_image_feature, image_feature), dim=0)\n",
    "                    else:\n",
    "                        image_feature = image_feature[0]\n",
    "                        if 'unpad' in mm_patch_merge_type:\n",
    "                            image_feature = torch.cat((\n",
    "                                image_feature,\n",
    "                                self.model.image_newline[None].to(image_feature.device)\n",
    "                            ), dim=0)\n",
    "                    new_image_features.append(image_feature)\n",
    "                image_features = new_image_features\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected mm_patch_merge_type: {self.config.mm_patch_merge_type}\")\n",
    "\n",
    "        # True\n",
    "        else:\n",
    "            logger.debug(f\"å˜ä¸€ã®ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ {images.shape}\")\n",
    "\n",
    "            # (1, 3, 336, 336) -> (1, 576, 4096)\n",
    "            image_features = self.encode_images(images)\n",
    "\n",
    "        # False\n",
    "        if getattr(self.config, 'tune_mm_mlp_adapter', False) and \\\n",
    "            getattr(self.config, 'mm_use_im_start_end', False):\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # 2) ãƒ©ãƒ™ãƒ«ãƒ»ä½ç½®IDãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å¿…è¦ã«å¿œã˜ã¦åˆæœŸåŒ–ã‚‚ã—ãã¯é©ç”¨\n",
    "\n",
    "        # å¤‰æ›´å‰ã®å„å¤‰æ•°ã‚’ä¿å­˜ï¼ˆNoneã®å ´åˆã¯ã‚ã¨ã§æ¨ã¦ã‚‹ï¼‰\n",
    "        _labels = labels\n",
    "        _position_ids = position_ids\n",
    "        _attention_mask = attention_mask\n",
    "\n",
    "        # åˆå›ï¼ˆprefillï¼‰ã¯Trueã§ã€2å›ç›®ä»¥é™ï¼ˆdecodeï¼‰ã¯False\n",
    "        if attention_mask is None:\n",
    "            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’1ã§åˆæœŸåŒ–\n",
    "            # (1, 59)\n",
    "            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "            logger.debug(f\"{attention_mask=}\")\n",
    "        else:\n",
    "            attention_mask = attention_mask.bool()\n",
    "\n",
    "        # åˆå›ï¼ˆprefillï¼‰ã¯Trueã§ã€2å›ç›®ä»¥é™ï¼ˆdecodeï¼‰ã¯False\n",
    "        if position_ids is None:\n",
    "            # ä½ç½®IDã‚’[0, 1, 2, ...]ã§åˆæœŸåŒ–\n",
    "            # (1, 59)\n",
    "            position_ids = torch.arange(\n",
    "                0,\n",
    "                input_ids.shape[1],\n",
    "                dtype=torch.long,\n",
    "                device=input_ids.device)\n",
    "\n",
    "        # æ¨è«–æ™‚ã¯å¸¸ã«True\n",
    "        if labels is None:\n",
    "            # ãƒ©ãƒ™ãƒ«ã‚’IGNORE_INDEXã§åˆæœŸåŒ–\n",
    "            # (1, 59)\n",
    "            labels = torch.full_like(input_ids, IGNORE_INDEX)\n",
    "\n",
    "        # å¤‰æ›´å‰ã®å…¥åŠ›IDã‚’ä¿å­˜\n",
    "        _input_ids = input_ids\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ã—ã¦å…¥åŠ›ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å‰Šé™¤\n",
    "        input_ids = [\n",
    "            cur_input_ids[cur_attention_mask] \\\n",
    "            for cur_input_ids, cur_attention_mask \\\n",
    "            in zip(input_ids, attention_mask)\n",
    "        ]\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ã—ã¦ãƒ©ãƒ™ãƒ«ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å‰Šé™¤\n",
    "        labels = [\n",
    "            cur_labels[cur_attention_mask] \\\n",
    "            for cur_labels, cur_attention_mask \\\n",
    "            in zip(labels, attention_mask)\n",
    "        ]\n",
    "\n",
    "        # 3) å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã«ç½®æ›\n",
    "\n",
    "        new_input_embeds = []\n",
    "        new_labels = []\n",
    "        cur_image_idx = 0\n",
    "\n",
    "        # å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã”ã¨ã«å‡¦ç†\n",
    "        for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "            logger.debug(f\"å‡¦ç†ä¸­ã®ãƒãƒƒãƒ {batch_idx=}, {cur_input_ids.shape=}\")\n",
    "\n",
    "            # ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "            # 1\n",
    "            num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n",
    "            logger.debug(f\"{batch_idx=}, {num_images=}\")\n",
    "\n",
    "            # ç”»åƒãŒå«ã¾ã‚Œãªã„å ´åˆã®å‡¦ç†\n",
    "            # False\n",
    "            if num_images == 0:\n",
    "                cur_image_features = image_features[cur_image_idx]\n",
    "                cur_input_embeds_1 = self.get_model().embed_tokens(cur_input_ids)\n",
    "                cur_input_embeds = torch.cat(\n",
    "                    [cur_input_embeds_1, cur_image_features[0:0]\n",
    "                ], dim=0)\n",
    "                new_input_embeds.append(cur_input_embeds)\n",
    "                new_labels.append(labels[batch_idx])\n",
    "                cur_image_idx += 1\n",
    "                continue\n",
    "\n",
    "            # ç”»åƒãŒå«ã¾ã‚Œã‚‹å ´åˆã®å‡¦ç†\n",
    "\n",
    "            # [-1, 35, 59]\n",
    "            image_token_indices = [-1] + \\\n",
    "                torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0].tolist() + \\\n",
    "                [cur_input_ids.shape[0]]\n",
    "            logger.debug(f\"{batch_idx=}, {image_token_indices=}\")\n",
    "\n",
    "            cur_input_ids_noim = []\n",
    "            cur_labels = labels[batch_idx]\n",
    "            cur_labels_noim = []\n",
    "\n",
    "            # ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "            for i in range(len(image_token_indices) - 1):\n",
    "\n",
    "                # ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ã„ãŸå…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "                cur_input_ids_noim.append(\n",
    "                    cur_input_ids[image_token_indices[i]+1:image_token_indices[i+1]]\n",
    "                )\n",
    "\n",
    "                # ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ã„ãŸãƒ©ãƒ™ãƒ«ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "                cur_labels_noim.append(\n",
    "                    cur_labels[image_token_indices[i]+1:image_token_indices[i+1]]\n",
    "                )\n",
    "\n",
    "            # ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ã„ãŸå„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "            split_sizes = [x.shape[0] for x in cur_labels_noim]\n",
    "\n",
    "            # ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ã„ãŸå…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«åŸ‹ã‚è¾¼ã¿ã‚’é©ç”¨\n",
    "            # (58, 4096)\n",
    "            cur_input_embeds = self.get_model().embed_tokens(\n",
    "                torch.cat(cur_input_ids_noim)\n",
    "            )\n",
    "            logger.debug(f\"{cur_input_embeds.shape=}\")\n",
    "\n",
    "            # ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ã„ãŸå…¥åŠ›åŸ‹ã‚è¾¼ã¿ã‚’åˆ†å‰²\n",
    "            # 2\n",
    "            cur_input_embeds_no_im = torch.split(\n",
    "                cur_input_embeds,\n",
    "                split_sizes,\n",
    "                dim=0\n",
    "            )\n",
    "            logger.debug(f\"{len(cur_input_embeds_no_im)=}\")\n",
    "\n",
    "            cur_new_input_embeds = []\n",
    "            cur_new_labels = []\n",
    "\n",
    "            # ç”»åƒã®æšæ•°+1å›ãƒ«ãƒ¼ãƒ—\n",
    "            for i in range(num_images + 1):\n",
    "\n",
    "                # ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®åŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ \n",
    "                cur_new_input_embeds.append(cur_input_embeds_no_im[i])\n",
    "\n",
    "                # ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ \n",
    "                cur_new_labels.append(cur_labels_noim[i])\n",
    "\n",
    "                # ç”»åƒ1æšã§æœ€åˆã®ãƒ«ãƒ¼ãƒ—ã®å ´åˆ\n",
    "                if i < num_images:\n",
    "\n",
    "                    # è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                    cur_image_features = image_features[cur_image_idx]\n",
    "                    cur_image_idx += 1\n",
    "                    cur_new_input_embeds.append(cur_image_features)\n",
    "\n",
    "                    # ç”»åƒéƒ¨åˆ†ã«å¯¾å¿œã™ã‚‹æ­£è§£ãƒ©ãƒ™ãƒ«ã¯IGNORE_INDEXï¼ˆ-100)\n",
    "                    cur_new_labels.append(\n",
    "                        torch.full((cur_image_features.shape[0],),\n",
    "                        IGNORE_INDEX,\n",
    "                        device=cur_labels.device,\n",
    "                        dtype=cur_labels.dtype))\n",
    "\n",
    "            # GPUã«è»¢é€\n",
    "            cur_new_input_embeds = [x.to(self.device) for x in cur_new_input_embeds]\n",
    "\n",
    "            # æ›´æ–°ã•ã‚ŒãŸå…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’çµåˆ\n",
    "            cur_new_input_embeds = torch.cat(cur_new_input_embeds)\n",
    "\n",
    "            # æ›´æ–°ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’çµåˆ\n",
    "            cur_new_labels = torch.cat(cur_new_labels)\n",
    "\n",
    "            # ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "            new_input_embeds.append(cur_new_input_embeds)\n",
    "\n",
    "            # ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "            new_labels.append(cur_new_labels)\n",
    "\n",
    "        # 4) è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŒ¿å…¥ã—ãŸå¾Œã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®èª¿æ•´\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æœ€å¤§é•·ã‚’å–å¾—\n",
    "        # None\n",
    "        tokenizer_model_max_length = getattr(\n",
    "            self.config,\n",
    "            'tokenizer_model_max_length',\n",
    "            None)\n",
    "        logger.debug(f\"{tokenizer_model_max_length=}\")\n",
    "\n",
    "        # æœ€å¤§é•·ãŒã‚ã‚‹å ´åˆ\n",
    "        # False\n",
    "        if tokenizer_model_max_length is not None:\n",
    "            new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]\n",
    "            new_labels = [x[:tokenizer_model_max_length] for x in new_labels]\n",
    "\n",
    "        # å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¤§é•·ã‚’è¨ˆç®—\n",
    "        max_len = max(x.shape[0] for x in new_input_embeds)\n",
    "        logger.debug(f\"{max_len=}\")\n",
    "\n",
    "        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "        batch_size = len(new_input_embeds)\n",
    "        logger.debug(f\"{batch_size=}\")\n",
    "\n",
    "        new_input_embeds_padded = []\n",
    "\n",
    "        # ãƒ©ãƒ™ãƒ«ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’-100ã§åˆæœŸåŒ–\n",
    "        new_labels_padded = torch.full(\n",
    "            (batch_size, max_len),\n",
    "            IGNORE_INDEX,\n",
    "            dtype=new_labels[0].dtype,\n",
    "            device=new_labels[0].device)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’0ã§åˆæœŸåŒ–\n",
    "        attention_mask = torch.zeros(\n",
    "            (batch_size, max_len),\n",
    "            dtype=attention_mask.dtype,\n",
    "            device=attention_mask.device)\n",
    "\n",
    "        # ä½ç½®IDã‚’0ã§åˆæœŸåŒ–\n",
    "        position_ids = torch.zeros(\n",
    "            (batch_size, max_len),\n",
    "            dtype=position_ids.dtype,\n",
    "            device=position_ids.device)\n",
    "\n",
    "        # å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨ãƒ©ãƒ™ãƒ«ã§ãƒ«ãƒ¼ãƒ—\n",
    "        for i, (cur_new_embed, cur_new_labels) in enumerate(zip(new_input_embeds, new_labels)):\n",
    "\n",
    "            # ç¾åœ¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’å–å¾—\n",
    "            cur_len = cur_new_embed.shape[0]\n",
    "            logger.debug(f\"{i=}, {cur_len=}\")\n",
    "\n",
    "            # å·¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®å ´åˆ\n",
    "            # False\n",
    "            if getattr(self.config, 'tokenizer_padding_side', 'right') == \"left\":\n",
    "                logger.debug(f\"å·¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° {i=}, {cur_len=}\")\n",
    "                new_input_embeds_padded.append(torch.cat((\n",
    "                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]), dtype=cur_new_embed.dtype, device=cur_new_embed.device),\n",
    "                    cur_new_embed\n",
    "                ), dim=0))\n",
    "                if cur_len > 0:\n",
    "                    new_labels_padded[i, -cur_len:] = cur_new_labels\n",
    "                    attention_mask[i, -cur_len:] = True\n",
    "                    position_ids[i, -cur_len:] = torch.arange(0, cur_len, dtype=position_ids.dtype, device=position_ids.device)\n",
    "\n",
    "            # å³ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®å ´åˆ\n",
    "            # True\n",
    "            else:\n",
    "                logger.debug(f\"å³ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° {i=}, {cur_len=}\")\n",
    "\n",
    "                # å³å´ã‚’ã‚¼ãƒ­ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°\n",
    "                new_input_embeds_padded.append(torch.cat((\n",
    "                    cur_new_embed,\n",
    "                    torch.zeros((max_len - cur_len, cur_new_embed.shape[1]),\n",
    "                    dtype=cur_new_embed.dtype,\n",
    "                    device=cur_new_embed.device)\n",
    "                ), dim=0))\n",
    "\n",
    "                # ç¾åœ¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒ0ã‚ˆã‚Šå¤§ãã„å ´åˆ\n",
    "                if cur_len > 0:\n",
    "                    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä¸Šæ›¸ã\n",
    "                    new_labels_padded[i, :cur_len] = cur_new_labels\n",
    "\n",
    "                    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’ä¸Šæ›¸ã\n",
    "                    attention_mask[i, :cur_len] = True\n",
    "\n",
    "                    # ä½ç½®IDã‚’ä¸Šæ›¸ã\n",
    "                    position_ids[i, :cur_len] = torch.arange(\n",
    "                        0,\n",
    "                        cur_len,\n",
    "                        dtype=position_ids.dtype,\n",
    "                        device=position_ids.device)\n",
    "\n",
    "        # ãƒªã‚¹ãƒˆã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n",
    "        logger.debug(f\"ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å¾Œã®å…¥åŠ›åŸ‹ã‚è¾¼ã¿ {new_input_embeds.shape=}\")\n",
    "\n",
    "        # 5) å‡ºåŠ›ã‚’æ•´å½¢\n",
    "\n",
    "        # ãƒ©ãƒ™ãƒ«ãŒã‚‚ã¨ã‚‚ã¨ãªã„å ´åˆã¯Noneã«æˆ»ã™\n",
    "        if _labels is None:\n",
    "            # Noneã«æˆ»ã™\n",
    "            new_labels = None\n",
    "        else:\n",
    "            # å‡ºåŠ›ã‚’æ–°ã—ã„ãƒ©ãƒ™ãƒ«ã«ç½®æ›\n",
    "            new_labels = new_labels_padded\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ãŒå…ƒã€…ç„¡ã„å ´åˆ\n",
    "        if _attention_mask is None:\n",
    "            # Noneã«æˆ»ã™\n",
    "            attention_mask = None\n",
    "        else:\n",
    "            # å‡ºåŠ›ã‚’æ–°ã—ã„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã«ç½®æ›\n",
    "            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n",
    "\n",
    "        # ä½ç½®IDãŒå…ƒã€…ç„¡ã„å ´åˆ\n",
    "        if _position_ids is None:\n",
    "            # Noneã«æˆ»ã™\n",
    "            position_ids = None\n",
    "        \n",
    "        logger.debug(f\"ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã®æº–å‚™å®Œäº† {new_input_embeds.shape=}, {position_ids=}, {attention_mask=}, {past_key_values=}, {new_labels=}\")\n",
    "\n",
    "        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels\n",
    "\n",
    "    def initialize_vision_tokenizer(self, model_args, tokenizer):\n",
    "        logger.info(f\"ãƒ“ã‚¸ãƒ§ãƒ³ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ– {model_args=}, {tokenizer=}\")\n",
    "        if model_args.mm_use_im_patch_token:\n",
    "            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
    "            self.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        if model_args.mm_use_im_start_end:\n",
    "            num_new_tokens = tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
    "            self.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "            if num_new_tokens > 0:\n",
    "                input_embeddings = self.get_input_embeddings().weight.data\n",
    "                output_embeddings = self.get_output_embeddings().weight.data\n",
    "\n",
    "                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
    "                    dim=0, keepdim=True)\n",
    "                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
    "                    dim=0, keepdim=True)\n",
    "\n",
    "                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "                output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "            if model_args.tune_mm_mlp_adapter:\n",
    "                for p in self.get_input_embeddings().parameters():\n",
    "                    p.requires_grad = True\n",
    "                for p in self.get_output_embeddings().parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "            if model_args.pretrain_mm_mlp_adapter:\n",
    "                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location='cpu')\n",
    "                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\n",
    "                assert num_new_tokens == 2\n",
    "                if input_embeddings.shape == embed_tokens_weight.shape:\n",
    "                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n",
    "                elif embed_tokens_weight.shape[0] == num_new_tokens:\n",
    "                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected embed_tokens_weight shape. Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. Numer of new tokens: {num_new_tokens}.\")\n",
    "        elif model_args.mm_use_im_patch_token:\n",
    "            if model_args.tune_mm_mlp_adapter:\n",
    "                for p in self.get_input_embeddings().parameters():\n",
    "                    p.requires_grad = False\n",
    "                for p in self.get_output_embeddings().parameters():\n",
    "                    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36974f74",
   "metadata": {},
   "source": [
    "### LlavaLlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014baa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaConfig(LlamaConfig):\n",
    "    \"\"\"\n",
    "    è¨€èªãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚¯ãƒ©ã‚¹ã‚’LLaVAç”¨ã«æ‹¡å¼µ\n",
    "    \"\"\"\n",
    "\n",
    "    model_type = \"llava_llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaLlamaModel(LlavaMetaModel, LlamaModel):\n",
    "    \"\"\"\n",
    "    LlavaMetaModelã¨LlamaModelã‚’çµ±åˆã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    LlavaLlamaForCausalLMã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = LlavaConfig\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "\n",
    "        logger.info(f\"LlavaLlamaModelã‚’åˆæœŸåŒ– {config=}\")\n",
    "\n",
    "        super(LlavaLlamaModel, self).__init__(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):\n",
    "    config_class = LlavaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(f\"LlavaLlamaForCausalLMã‚’åˆæœŸåŒ– {config=}\")\n",
    "\n",
    "        # https://huggingface.co/liuhaotian/llava-v1.5-7b/blob/main/config.json\n",
    "        super(LlamaForCausalLM, self).__init__(config)\n",
    "\n",
    "        # LlavaLlamaModelã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
    "        self.model = LlavaLlamaModel(config)\n",
    "\n",
    "        # Tensor Parallelismã®è¨­å®š\n",
    "        # 1ï¼ˆTPã‚’ä½¿ç”¨ã—ãªã„ï¼‰\n",
    "        self.pretraining_tp = config.pretraining_tp\n",
    "\n",
    "        # èªå½™æ•°ã®è¨­å®š\n",
    "        # 32000\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        # è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç·šå½¢å±¤ã‚’åˆæœŸåŒ–\n",
    "        # 4096 -> 32,000\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # é‡ã¿ã‚’åˆæœŸåŒ–ã—ã€æœ€çµ‚å‡¦ç†ã‚’é©ç”¨\n",
    "        self.post_init()\n",
    "\n",
    "    def get_model(self):\n",
    "        \"LlavaLlamaModelã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—\"\n",
    "        return self.model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        images: Optional[torch.FloatTensor] = None,\n",
    "        image_sizes: Optional[List[List[int]]] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position=None, # https://github.com/huggingface/transformers/issues/29426\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        \"\"\"\n",
    "        ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "        \"\"\"\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            (\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                past_key_values,\n",
    "                inputs_embeds,\n",
    "                labels\n",
    "            ) = self.prepare_inputs_labels_for_multimodal(\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                past_key_values,\n",
    "                labels,\n",
    "                images,\n",
    "                image_sizes\n",
    "            )\n",
    "\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        inputs: Optional[torch.Tensor] = None,\n",
    "        images: Optional[torch.Tensor] = None,\n",
    "        image_sizes: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹\n",
    "        eval_modelé–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            inputs: ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            images: ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã§å‰å‡¦ç†æ¸ˆã¿ã®ç”»åƒå…¥åŠ›ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            image_sizes: å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã®ãƒªã‚¹ãƒˆ\n",
    "            **kwargs:\n",
    "                do_sample: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ã‹ã©ã†ã‹\n",
    "                temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "                top_p: nucleusã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ç¢ºç‡é–¾å€¤\n",
    "                num_beams: ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã®ãƒ“ãƒ¼ãƒ æ•°\n",
    "                max_new_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "                use_cache: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "                ...\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹ {inputs.shape=}, {images.shape=}, {image_sizes=}, {kwargs=}\")\n",
    "\n",
    "        # 1) ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŠ½å‡º\n",
    "\n",
    "        # None\n",
    "        position_ids = kwargs.pop(\"position_ids\", None)\n",
    "\n",
    "        # None\n",
    "        attention_mask = kwargs.pop(\"attention_mask\", None)\n",
    "\n",
    "        # None\n",
    "        if \"inputs_embeds\" in kwargs:\n",
    "            raise NotImplementedError(\"`inputs_embeds` is not supported\")\n",
    "\n",
    "        # 2) ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã®æº–å‚™\n",
    "\n",
    "        # ç”»åƒãŒã‚ã‚‹å ´åˆ\n",
    "        # True\n",
    "        if images is not None:\n",
    "\n",
    "            # ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®<image>ã®ä½ç½®ã«æŒ¿å…¥\n",
    "            # ãƒ‘ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã‚’èª¿æ•´ã—ã€ãã‚Œã«å¿œã˜ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã¨ä½ç½®IDã‚’æ›´æ–°\n",
    "            (\n",
    "                inputs,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                _,\n",
    "                inputs_embeds,\n",
    "                _\n",
    "            ) = self.prepare_inputs_labels_for_multimodal(\n",
    "                inputs,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                None,\n",
    "                None,\n",
    "                images,\n",
    "                image_sizes=image_sizes\n",
    "            )\n",
    "\n",
    "        # False\n",
    "        else:\n",
    "            inputs_embeds = self.get_model().embed_tokens(inputs)\n",
    "\n",
    "        # LlamaForCausalLMã®generateãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™\n",
    "        result = super().generate(\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            **kwargs\n",
    "        )\n",
    "        logger.debug(f\"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆçµæœ {result=}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, inputs_embeds=None, **kwargs):\n",
    "        \"\"\"\n",
    "        LlamaForCausalLMã®generateãƒ¡ã‚½ãƒƒãƒ‰ã®å†…éƒ¨ãƒ«ãƒ¼ãƒ—ã§æ¯å›å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "        LlamaForCausalLMç”¨ã«å…¥åŠ›ã‚’æ•´å½¢ã™ã‚‹\n",
    "        imagesã¨ã„ã†å¼•æ•°ãŒæ­£ã—ãæ¸¡ã•ã‚Œã‚‹ã‚ˆã†ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰\n",
    "\n",
    "        Args:\n",
    "            input_ids: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            past_key_values: éå»ã®ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            inputs_embeds: ãƒ†ã‚­ã‚¹ãƒˆã®å…¥åŠ›åŸ‹ã‚è¾¼ã¿ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            **kwargs: ãã®ä»–ã®å¼•æ•°\n",
    "        Returns:\n",
    "            æ•´å½¢ã•ã‚ŒãŸå…¥åŠ›ã®è¾æ›¸\n",
    "        \"\"\"\n",
    "        images = kwargs.pop(\"images\", None)\n",
    "        image_sizes = kwargs.pop(\"image_sizes\", None)\n",
    "\n",
    "        logger.info(f\"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç”¨ã«å…¥åŠ›ã‚’æ•´å½¢ {input_ids.shape=}, {past_key_values=}, {inputs_embeds.shape if inputs_embeds is not None else None=}, {images.shape if images is not None else None=}, {image_sizes=}, {kwargs.keys()=}\")\n",
    "\n",
    "        # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å‰ã®prepare_inputs_for_generationã‚’å‘¼ã³å‡ºã™\n",
    "        inputs = super().prepare_inputs_for_generation(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        if images is not None:\n",
    "            inputs['images'] = images\n",
    "\n",
    "        if image_sizes is not None:\n",
    "            inputs['image_sizes'] = image_sizes\n",
    "\n",
    "        logger.debug(f\"æ•´å½¢å¾Œã®å…¥åŠ› {inputs.keys()=}\")\n",
    "\n",
    "        return inputs\n",
    "\n",
    "AutoConfig.register(\"llava_llama\", LlavaConfig)\n",
    "AutoModelForCausalLM.register(LlavaConfig, LlavaLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d9da9",
   "metadata": {},
   "source": [
    "### äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa26e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name_from_path(model_path):\n",
    "    model_path = model_path.strip(\"/\")\n",
    "    model_paths = model_path.split(\"/\")\n",
    "    if model_paths[-1].startswith('checkpoint-'):\n",
    "        result = model_paths[-2] + \"_\" + model_paths[-1]\n",
    "    else:\n",
    "        result = model_paths[-1]\n",
    "\n",
    "    logger.info(f\"ãƒ¢ãƒ‡ãƒ«åã‚’ãƒ‘ã‚¹ã‹ã‚‰æŠ½å‡º {result=}\")\n",
    "    return result\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False, device_map=\"auto\", device=\"cuda\", use_flash_attn=False, **kwargs):\n",
    "    \"\"\"\n",
    "    äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "    Args:\n",
    "        model_path: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹\n",
    "        model_base: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹\n",
    "        model_name: ãƒ¢ãƒ‡ãƒ«å\n",
    "        load_8bit: 8ãƒ“ãƒƒãƒˆã§ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ•ãƒ©ã‚°\n",
    "        load_4bit: 4ãƒ“ãƒƒãƒˆã§ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ•ãƒ©ã‚°\n",
    "        device_map: ãƒ‡ãƒã‚¤ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°è¨­å®š\n",
    "        device: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹\n",
    "        use_flash_attn: Flash Attentionã‚’ä½¿ç”¨ã™ã‚‹ãƒ•ãƒ©ã‚°\n",
    "        **kwargs: ãã®ä»–ã®å¼•æ•°\n",
    "    Returns:\n",
    "        ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ {model_path=}, {model_base=}, {model_name=}, {load_8bit=}, {load_4bit=}, {device_map=}, {device=}, {use_flash_attn=}, {kwargs=}\")\n",
    "\n",
    "    kwargs = {\"device_map\": device_map, **kwargs}\n",
    "\n",
    "    # False\n",
    "    if device != \"cuda\":\n",
    "        kwargs['device_map'] = {\"\": device}\n",
    "\n",
    "    # False\n",
    "    if load_8bit:\n",
    "        kwargs['load_in_8bit'] = True\n",
    "\n",
    "    # False\n",
    "    elif load_4bit:\n",
    "        kwargs['load_in_4bit'] = True\n",
    "        kwargs['quantization_config'] = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4'\n",
    "        )\n",
    "\n",
    "    # True\n",
    "    else:\n",
    "        kwargs['torch_dtype'] = torch.float16\n",
    "\n",
    "    # False\n",
    "    if use_flash_attn:\n",
    "        kwargs['attn_implementation'] = 'flash_attention_2'\n",
    "\n",
    "    # True\n",
    "    if 'llava' in model_name.lower():\n",
    "\n",
    "        logger.debug(f\"é€šå¸¸ã®LLaVAãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\")\n",
    "    \n",
    "        # False\n",
    "        if 'lora' in model_name.lower() and model_base is None:\n",
    "            logger.warning('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')\n",
    "\n",
    "        # False\n",
    "        if 'lora' in model_name.lower() and model_base is not None:\n",
    "            from llava.model.language_model.llava_llama import LlavaConfig\n",
    "            lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n",
    "            print('Loading LLaVA from base model...')\n",
    "            model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)\n",
    "            token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features\n",
    "            if model.lm_head.weight.shape[0] != token_num:\n",
    "                model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n",
    "                model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))\n",
    "\n",
    "            print('Loading additional LLaVA weights...')\n",
    "            if os.path.exists(os.path.join(model_path, 'non_lora_trainables.bin')):\n",
    "                non_lora_trainables = torch.load(os.path.join(model_path, 'non_lora_trainables.bin'), map_location='cpu')\n",
    "            else:\n",
    "                # this is probably from HF Hub\n",
    "                from huggingface_hub import hf_hub_download\n",
    "                def load_from_hf(repo_id, filename, subfolder=None):\n",
    "                    cache_file = hf_hub_download(\n",
    "                        repo_id=repo_id,\n",
    "                        filename=filename,\n",
    "                        subfolder=subfolder)\n",
    "                    return torch.load(cache_file, map_location='cpu')\n",
    "                non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')\n",
    "            non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}\n",
    "            if any(k.startswith('model.model.') for k in non_lora_trainables):\n",
    "                non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}\n",
    "            model.load_state_dict(non_lora_trainables, strict=False)\n",
    "\n",
    "            from peft import PeftModel\n",
    "            print('Loading LoRA weights...')\n",
    "            model = PeftModel.from_pretrained(model, model_path)\n",
    "            print('Merging LoRA weights...')\n",
    "            model = model.merge_and_unload()\n",
    "            print('Model is loaded...')\n",
    "\n",
    "        # False\n",
    "        elif model_base is not None:\n",
    "            # this may be mm projector only\n",
    "            if 'mpt' in model_name.lower():\n",
    "                if not os.path.isfile(os.path.join(model_path, 'configuration_mpt.py')):\n",
    "                    shutil.copyfile(os.path.join(model_base, 'configuration_mpt.py'), os.path.join(model_path, 'configuration_mpt.py'))\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)\n",
    "                cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "                model = LlavaMptForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    model_base,\n",
    "                    use_fast=False\n",
    "                )\n",
    "                cfg_pretrained = AutoConfig.from_pretrained(model_path)\n",
    "                model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "                    model_base,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    config=cfg_pretrained,\n",
    "                    **kwargs)\n",
    "\n",
    "            mm_projector_weights = torch.load(\n",
    "                os.path.join(model_path, 'mm_projector.bin'),\n",
    "                map_location='cpu'\n",
    "            )\n",
    "\n",
    "            mm_projector_weights = {\n",
    "                k: v.to(torch.float16) for k, v in mm_projector_weights.items()\n",
    "            }\n",
    "\n",
    "            model.load_state_dict(mm_projector_weights, strict=False)\n",
    "\n",
    "        # True\n",
    "        else:\n",
    "\n",
    "            # False\n",
    "            if 'mpt' in model_name.lower():\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "                model = LlavaMptForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n",
    "\n",
    "            # False\n",
    "            elif 'mistral' in model_name.lower():\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "                model = LlavaMistralForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    **kwargs\n",
    "                )\n",
    "\n",
    "            # True\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    model_path,\n",
    "                    use_fast=False\n",
    "                )\n",
    "                logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ {tokenizer=} {model_path}\")\n",
    "\n",
    "                model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    **kwargs\n",
    "                )\n",
    "                logger.debug(f\"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ {model=} {model_path=}\")\n",
    "    else:\n",
    "        # Load language model\n",
    "\n",
    "        if model_base is not None:\n",
    "            # PEFT model\n",
    "            from peft import PeftModel\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)\n",
    "            print(f\"Loading LoRA weights from {model_path}\")\n",
    "            model = PeftModel.from_pretrained(model, model_path)\n",
    "            print(f\"Merging weights\")\n",
    "            model = model.merge_and_unload()\n",
    "            print('Convert to FP16...')\n",
    "            model.to(torch.float16)\n",
    "        else:\n",
    "            use_fast = False\n",
    "            if 'mpt' in model_name.lower():\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)\n",
    "            else:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)\n",
    "\n",
    "    # ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "    image_processor = None\n",
    "\n",
    "    # True\n",
    "    if 'llava' in model_name.lower():\n",
    "\n",
    "        # False\n",
    "        mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n",
    "\n",
    "        # False\n",
    "        mm_use_im_patch_token = getattr(model.config, \"mm_use_im_patch_token\", True)\n",
    "\n",
    "        # False\n",
    "        if mm_use_im_patch_token:\n",
    "            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n",
    "\n",
    "        # False\n",
    "        if mm_use_im_start_end:\n",
    "            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™ã‚µã‚¤ã‚ºã«åˆã‚ã›å…¥åŠ›å±¤ã¨å‡ºåŠ›å±¤ã‚’ãƒªã‚µã‚¤ã‚º\n",
    "        # 32000 -> 32000\n",
    "        # https://huggingface.co/docs/transformers/en/main_classes/model#transformers.PreTrainedModel.resize_token_embeddings\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        vision_tower = model.get_vision_tower()\n",
    "\n",
    "        # True\n",
    "        if not vision_tower.is_loaded:\n",
    "\n",
    "            # ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ãƒ¯ãƒ¼ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            vision_tower.load_model(device_map=device_map)\n",
    "\n",
    "        # False\n",
    "        if device_map != 'auto':\n",
    "            vision_tower.to(device=device_map, dtype=torch.float16)\n",
    "\n",
    "        # CLIPVisionModelå…¥åŠ›ã®å‰å‡¦ç†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        image_processor = vision_tower.image_processor\n",
    "\n",
    "    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ±ºå®š\n",
    "\n",
    "    # False\n",
    "    if hasattr(model.config, \"max_sequence_length\"):\n",
    "        context_len = model.config.max_sequence_length\n",
    "    else:\n",
    "        context_len = 2048\n",
    "\n",
    "    logger.debug(f\"{context_len=}\")\n",
    "\n",
    "    return tokenizer, model, image_processor, context_len\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2e3bf",
   "metadata": {},
   "source": [
    "### Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROLLER_HEART_BEAT_EXPIRATION = 30\n",
    "WORKER_HEART_BEAT_INTERVAL = 15\n",
    "\n",
    "LOGDIR = \".\"\n",
    "\n",
    "# Model Constants\n",
    "IGNORE_INDEX = -100\n",
    "IMAGE_TOKEN_INDEX = -200\n",
    "DEFAULT_IMAGE_TOKEN = \"<image>\"\n",
    "DEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\n",
    "DEFAULT_IM_START_TOKEN = \"<im_start>\"\n",
    "DEFAULT_IM_END_TOKEN = \"<im_end>\"\n",
    "IMAGE_PLACEHOLDER = \"<image-placeholder>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparatorStyle(Enum):\n",
    "    \"\"\"\n",
    "    ãƒãƒ£ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®åŒºåˆ‡ã‚Šæ–¹\n",
    "    \"\"\"\n",
    "    SINGLE = auto() # sep=\"###\"\n",
    "    TWO = auto() # sep=\" \", sep2=\"</s>\"\n",
    "    MPT = auto() \n",
    "    PLAIN = auto()\n",
    "    LLAMA_2 = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de25f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Conversation:\n",
    "    \"\"\"\n",
    "    ãƒãƒ£ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "    system: str\n",
    "\n",
    "    # ãƒ­ãƒ¼ãƒ«ï¼ˆUSER, ASSISTANT)\n",
    "    roles: List[str]\n",
    "\n",
    "    # [[role, message], ...]\n",
    "    messages: List[List[str]]\n",
    "\n",
    "    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºé–‹å§‹ä½ç½®\n",
    "    offset: int\n",
    "\n",
    "    # åŒºåˆ‡ã‚Šã‚¹ã‚¿ã‚¤ãƒ«\n",
    "    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n",
    "\n",
    "    # åŒºåˆ‡ã‚Šæ–‡å­—1\n",
    "    sep: str = \"###\"\n",
    "\n",
    "    # åŒºåˆ‡ã‚Šæ–‡å­—2\n",
    "    sep2: str = None\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³\n",
    "    version: str = \"Unknown\"\n",
    "\n",
    "    skip_next: bool = False\n",
    "\n",
    "    def get_prompt(self):\n",
    "        \"\"\"\n",
    "        è¿½åŠ ã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒãƒ£ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã—ã¦è¿”ã™\n",
    "\n",
    "        Returns:\n",
    "            æ§‹ç¯‰ã•ã‚ŒãŸãƒãƒ£ãƒƒãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ–‡å­—åˆ—\n",
    "        ä¾‹:\n",
    "            A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
    "            ### USER: <image>\n",
    "            What are the things you can see in this image?\n",
    "            ### ASSISTANT: This is a photo of a cat sitting on a windowsill looking outside. The cat appears to be a tabby with a mix of brown, black, and white fur. Outside the window, there are green trees and a bright sky, indicating that it is likely daytime.\n",
    "            ### USER: Can you describe the cat's fur pattern?\n",
    "            ### ASSISTANT:\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "        messages = self.messages\n",
    "\n",
    "        logger.info(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰é–‹å§‹ {messages=}, {self.sep_style=}, {self.version=}\")\n",
    "\n",
    "        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç”»åƒã¤ãã®å ´åˆ\n",
    "        # False\n",
    "        if len(messages) > 0 and type(messages[0][1]) is tuple:\n",
    "            messages = self.messages.copy()\n",
    "            init_role, init_msg = messages[0].copy()\n",
    "            init_msg = init_msg[0].replace(\"<image>\", \"\").strip()\n",
    "            if 'mmtag' in self.version:\n",
    "                messages[0] = (init_role, init_msg)\n",
    "                messages.insert(0, (self.roles[0], \"<Image><image></Image>\"))\n",
    "                messages.insert(1, (self.roles[1], \"Received.\"))\n",
    "            else:\n",
    "                messages[0] = (init_role, \"<image>\\n\" + init_msg)\n",
    "\n",
    "        # False\n",
    "        if self.sep_style == SeparatorStyle.SINGLE:\n",
    "            ret = self.system + self.sep\n",
    "            for role, message in messages:\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    ret += role + \": \" + message + self.sep\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "\n",
    "        # True\n",
    "        elif self.sep_style == SeparatorStyle.TWO:\n",
    "\n",
    "            # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "            # [\" \", \"</s>\"]\n",
    "            seps = [self.sep, self.sep2]\n",
    "\n",
    "            # æˆ»ã‚Šå€¤ã«ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ \n",
    "            # \"A chat between a curious human...\"\" + \" \"\n",
    "            ret = self.system + seps[0]\n",
    "\n",
    "            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã§ãƒ«ãƒ¼ãƒ—\n",
    "            for i, (role, message) in enumerate(messages):\n",
    "\n",
    "                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "                if message:\n",
    "\n",
    "                    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚¿ãƒ—ãƒ«ã®å ´åˆã¯æŠ½å‡º\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "\n",
    "                    # iãŒå¶æ•°: \"USER: <image>\\nWhat are the thing...\" + \" \"\n",
    "                    # iãŒå¥‡æ•°: \"ASSISTANT: This is a photo...\" + \"</s>\"\n",
    "                    ret += role + \": \" + message + seps[i % 2]\n",
    "\n",
    "                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå­˜åœ¨ã—ãªã„å ´åˆï¼ˆç”Ÿæˆå¯¾è±¡ï¼‰\n",
    "                else:\n",
    "                    # \"ASSISTANT:\"ã‚’æˆ»ã‚Šå€¤ã«è¿½åŠ \n",
    "                    ret += role + \":\"\n",
    "\n",
    "        # False\n",
    "        elif self.sep_style == SeparatorStyle.MPT:\n",
    "            ret = self.system + self.sep\n",
    "            for role, message in messages:\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    ret += role + message + self.sep\n",
    "                else:\n",
    "                    ret += role\n",
    "\n",
    "        # False\n",
    "        elif self.sep_style == SeparatorStyle.LLAMA_2:\n",
    "            wrap_sys = lambda msg: f\"<<SYS>>\\n{msg}\\n<</SYS>>\\n\\n\" if len(msg) > 0 else msg\n",
    "            wrap_inst = lambda msg: f\"[INST] {msg} [/INST]\"\n",
    "            ret = \"\"\n",
    "\n",
    "            for i, (role, message) in enumerate(messages):\n",
    "                if i == 0:\n",
    "                    assert message, \"first message should not be none\"\n",
    "                    assert role == self.roles[0], \"first message should come from user\"\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    if i == 0: message = wrap_sys(self.system) + message\n",
    "                    if i % 2 == 0:\n",
    "                        message = wrap_inst(message)\n",
    "                        ret += self.sep + message\n",
    "                    else:\n",
    "                        ret += \" \" + message + \" \" + self.sep2\n",
    "                else:\n",
    "                    ret += \"\"\n",
    "            ret = ret.lstrip(self.sep)\n",
    "\n",
    "        # False\n",
    "        elif self.sep_style == SeparatorStyle.PLAIN:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = self.system\n",
    "            for i, (role, message) in enumerate(messages):\n",
    "                if message:\n",
    "                    if type(message) is tuple:\n",
    "                        message, _, _ = message\n",
    "                    ret += message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += \"\"\n",
    "\n",
    "        # False\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid style: {self.sep_style}\")\n",
    "\n",
    "        logger.info(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰å®Œäº† {ret=}\")\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def append_message(self, role, message):\n",
    "        \"\"\"\n",
    "        Conversationã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã™ã‚‹\n",
    "        eval_modelé–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            role: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ­ãƒ¼ãƒ«ï¼ˆUSER, ASSISTANT)\n",
    "            message: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å†…å®¹\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ  {role=}, {message=}\")\n",
    "        self.messages.append([role, message])\n",
    "\n",
    "    def process_image(self, image, image_process_mode, return_pil=False, image_format='PNG', max_len=1344, min_len=672):\n",
    "        \"\"\"\n",
    "        ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "        \"\"\"\n",
    "        if image_process_mode == \"Pad\":\n",
    "            def expand2square(pil_img, background_color=(122, 116, 104)):\n",
    "                width, height = pil_img.size\n",
    "                if width == height:\n",
    "                    return pil_img\n",
    "                elif width > height:\n",
    "                    result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "                    result.paste(pil_img, (0, (width - height) // 2))\n",
    "                    return result\n",
    "                else:\n",
    "                    result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "                    result.paste(pil_img, ((height - width) // 2, 0))\n",
    "                    return result\n",
    "            image = expand2square(image)\n",
    "        elif image_process_mode in [\"Default\", \"Crop\"]:\n",
    "            pass\n",
    "        elif image_process_mode == \"Resize\":\n",
    "            image = image.resize((336, 336))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid image_process_mode: {image_process_mode}\")\n",
    "        if max(image.size) > max_len:\n",
    "            max_hw, min_hw = max(image.size), min(image.size)\n",
    "            aspect_ratio = max_hw / min_hw\n",
    "            shortest_edge = int(min(max_len / aspect_ratio, min_len, min_hw))\n",
    "            longest_edge = int(shortest_edge * aspect_ratio)\n",
    "            W, H = image.size\n",
    "            if H > W:\n",
    "                H, W = longest_edge, shortest_edge\n",
    "            else:\n",
    "                H, W = shortest_edge, longest_edge\n",
    "            image = image.resize((W, H))\n",
    "        if return_pil:\n",
    "            return image\n",
    "        else:\n",
    "            buffered = BytesIO()\n",
    "            image.save(buffered, format=image_format)\n",
    "            img_b64_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "            return img_b64_str\n",
    "\n",
    "    def get_images(self, return_pil=False):\n",
    "        \"\"\"\n",
    "        ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "        \"\"\"\n",
    "        images = []\n",
    "        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n",
    "            if i % 2 == 0:\n",
    "                if type(msg) is tuple:\n",
    "                    msg, image, image_process_mode = msg\n",
    "                    image = self.process_image(image, image_process_mode, return_pil=return_pil)\n",
    "                    images.append(image)\n",
    "        return images\n",
    "\n",
    "    def to_gradio_chatbot(self):\n",
    "        \"\"\"\n",
    "        ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n",
    "            if i % 2 == 0:\n",
    "                if type(msg) is tuple:\n",
    "                    msg, image, image_process_mode = msg\n",
    "                    img_b64_str = self.process_image(\n",
    "                        image, \"Default\", return_pil=False,\n",
    "                        image_format='JPEG')\n",
    "                    img_str = f'<img src=\"data:image/jpeg;base64,{img_b64_str}\" alt=\"user upload image\" />'\n",
    "                    msg = img_str + msg.replace('<image>', '').strip()\n",
    "                    ret.append([msg, None])\n",
    "                else:\n",
    "                    ret.append([msg, None])\n",
    "            else:\n",
    "                ret[-1][-1] = msg\n",
    "        return ret\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"\n",
    "        ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "        \"\"\"\n",
    "        return Conversation(\n",
    "            system=self.system,\n",
    "            roles=self.roles,\n",
    "            messages=[[x, y] for x, y in self.messages],\n",
    "            offset=self.offset,\n",
    "            sep_style=self.sep_style,\n",
    "            sep=self.sep,\n",
    "            sep2=self.sep2,\n",
    "            version=self.version)\n",
    "\n",
    "    def dict(self):\n",
    "        \"\"\"\n",
    "        ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "        \"\"\"\n",
    "        if len(self.get_images()) > 0:\n",
    "            return {\n",
    "                \"system\": self.system,\n",
    "                \"roles\": self.roles,\n",
    "                \"messages\": [[x, y[0] if type(y) is tuple else y] for x, y in self.messages],\n",
    "                \"offset\": self.offset,\n",
    "                \"sep\": self.sep,\n",
    "                \"sep2\": self.sep2,\n",
    "            }\n",
    "        return {\n",
    "            \"system\": self.system,\n",
    "            \"roles\": self.roles,\n",
    "            \"messages\": self.messages,\n",
    "            \"offset\": self.offset,\n",
    "            \"sep\": self.sep,\n",
    "            \"sep2\": self.sep2,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaVA v1 ãƒãƒ£ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å®šç¾©\n",
    "\n",
    "conv_llava_v1 = Conversation(\n",
    "    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n",
    "           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n",
    "    roles=(\"USER\", \"ASSISTANT\"),\n",
    "    version=\"v1\",\n",
    "    messages=(),\n",
    "    offset=0,\n",
    "    sep_style=SeparatorStyle.TWO,\n",
    "    sep=\" \",\n",
    "    sep2=\"</s>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒãƒ£ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®è¾æ›¸\n",
    "# eval_modelé–¢æ•°ã§ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "\n",
    "conv_templates = {\n",
    "    # \"default\": conv_vicuna_v0,\n",
    "    # \"v0\": conv_vicuna_v0,\n",
    "    # \"v1\": conv_vicuna_v1,\n",
    "    # \"vicuna_v1\": conv_vicuna_v1,\n",
    "    # \"llama_2\": conv_llama_2,\n",
    "    # \"mistral_instruct\": conv_mistral_instruct,\n",
    "    # \"chatml_direct\": conv_chatml_direct,\n",
    "    # \"mistral_direct\": conv_chatml_direct,\n",
    "\n",
    "    # \"plain\": conv_llava_plain,\n",
    "    # \"v0_plain\": conv_llava_plain,\n",
    "    # \"llava_v0\": conv_llava_v0,\n",
    "    # \"v0_mmtag\": conv_llava_v0_mmtag,\n",
    "    \"llava_v1\": conv_llava_v1,\n",
    "    # \"v1_mmtag\": conv_llava_v1_mmtag,\n",
    "    # \"llava_llama_2\": conv_llava_llama_2,\n",
    "\n",
    "    # \"mpt\": conv_mpt,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a1a21",
   "metadata": {},
   "source": [
    "### è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_torch_init():\n",
    "    \"\"\"\n",
    "    PyTorchãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–\n",
    "    ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚’é«˜é€ŸåŒ–ã™ã‚‹\n",
    "    eval_modelé–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "    setattr(torch.nn.Linear, \"reset_parameters\", lambda self: None)\n",
    "    setattr(torch.nn.LayerNorm, \"reset_parameters\", lambda self: None)\n",
    "    logger.info(\"PyTorchãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_parser(args):\n",
    "    \"\"\"\n",
    "    ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆ,ãªã©ï¼‰ã§åŒºåˆ‡ã‚‰ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦å–å¾—\n",
    "    eval_modelã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "    out = args.image_file.split(args.sep)\n",
    "    logger.info(f\"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾— {out=}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_file):\n",
    "    \"\"\"\n",
    "    ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’PILç”»åƒã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰\n",
    "    URLã®å ´åˆã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰\n",
    "    load_imagesé–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "    Args:\n",
    "        image_file: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¾ãŸã¯URL\n",
    "    Returns:\n",
    "        PILç”»åƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒURLã®å ´åˆ\n",
    "    # True\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦PILç”»åƒã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰\n",
    "        # https://llava-vl.github.io/static/images/view.jpg\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "\n",
    "    logger.info(f\"PILç”»åƒã‚’ãƒ­ãƒ¼ãƒ‰ {image_file=}, {image.size=}, {image.mode=}\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_files):\n",
    "    \"\"\"\n",
    "    ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã‹ã‚‰PILç”»åƒã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "    eval_modelé–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "    Args:\n",
    "        image_files: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "    Returns:\n",
    "        PILç”»åƒã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f369136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_anyres_image(image, processor, grid_pinpoints):\n",
    "    \"\"\"\n",
    "    ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "\n",
    "    Process an image with variable resolutions.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): The input image to be processed.\n",
    "        processor: The image processor object.\n",
    "        grid_pinpoints (str): A string representation of a list of possible resolutions.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the processed image patches.\n",
    "    \"\"\"\n",
    "    if type(grid_pinpoints) is list:\n",
    "        possible_resolutions = grid_pinpoints\n",
    "    else:\n",
    "        possible_resolutions = ast.literal_eval(grid_pinpoints)\n",
    "    best_resolution = select_best_resolution(image.size, possible_resolutions)\n",
    "    image_padded = resize_and_pad_image(image, best_resolution)\n",
    "\n",
    "    patches = divide_to_patches(image_padded, processor.crop_size['height'])\n",
    "\n",
    "    image_original_resize = image.resize((processor.size['shortest_edge'], processor.size['shortest_edge']))\n",
    "\n",
    "    image_patches = [image_original_resize] + patches\n",
    "    image_patches = [processor.preprocess(image_patch, return_tensors='pt')['pixel_values'][0]\n",
    "                     for image_patch in image_patches]\n",
    "    return torch.stack(image_patches, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand2square(pil_img, background_color):\n",
    "    \"\"\"\n",
    "    PILç”»åƒã‚’æ­£æ–¹å½¢ã«æ‹¡å¼µã™ã‚‹\n",
    "    process_imagesé–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "    Args:\n",
    "        pil_img (PIL.Image.Image): å…¥åŠ›PILç”»åƒ\n",
    "        background_color (tuple): èƒŒæ™¯è‰²ã®RGBã‚¿ãƒ—ãƒ«\n",
    "    Returns:\n",
    "        PIL.Image.Image: æ­£æ–¹å½¢ã«æ‹¡å¼µã•ã‚ŒãŸPILç”»åƒ\n",
    "    \"\"\"\n",
    "    logger.info(f\"PILç”»åƒã‚’æ­£æ–¹å½¢ã«æ‹¡å¼µ {pil_img.size=}, {background_color=}\")\n",
    "\n",
    "    # 1000, 667\n",
    "    width, height = pil_img.size\n",
    "\n",
    "    # False\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "\n",
    "    # True\n",
    "    elif width > height:\n",
    "\n",
    "        # (255, 255, 255)ã®èƒŒæ™¯è‰²ã§æ­£æ–¹å½¢ã®æ–°ã—ã„ç”»åƒã‚’ä½œæˆ\n",
    "        # 1000x1000\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "\n",
    "        # ä¸­å¤®ã«å…ƒã®ç”»åƒã‚’è²¼ã‚Šä»˜ã‘\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "\n",
    "        return result\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(images, image_processor, model_cfg):\n",
    "    \"\"\"\n",
    "    PILç”»åƒã‚’å‰å‡¦ç†ã—ã€ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã™ã‚‹\n",
    "    eval_modelé–¢æ•°ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "    Args:\n",
    "        images (list of PIL.Image.Image): å…¥åŠ›PILç”»åƒã®ãƒªã‚¹ãƒˆ\n",
    "        image_processor: ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        model_cfg: ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "    Returns:\n",
    "        torch.Tensor: å‰å‡¦ç†ã•ã‚ŒãŸç”»åƒãƒ†ãƒ³ã‚½ãƒ«\n",
    "    \"\"\"\n",
    "    logger.info(f\"ç”»åƒã‚’å‰å‡¦ç† {len(images)=}, {model_cfg.image_aspect_ratio=}\")\n",
    "\n",
    "    # \"pad\"\n",
    "    image_aspect_ratio = getattr(model_cfg, \"image_aspect_ratio\", None)\n",
    "\n",
    "    new_images = []\n",
    "\n",
    "\n",
    "    # æ­£æ–¹å½¢ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹å ´åˆ\n",
    "    # True\n",
    "    if image_aspect_ratio == 'pad':\n",
    "\n",
    "        # ç”»åƒã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "        for image in images:\n",
    "\n",
    "            # æ­£æ–¹å½¢ã«æ‹¡å¼µ\n",
    "            image = expand2square(\n",
    "                image,\n",
    "                tuple(int(x*255) for x in image_processor.image_mean) # èƒŒæ™¯è‰²\n",
    "            )\n",
    "\n",
    "            # ç”»åƒã‚’å‰å‡¦ç†\n",
    "            image = image_processor.preprocess(\n",
    "                image,\n",
    "                return_tensors='pt'\n",
    "            )['pixel_values'][0]\n",
    "\n",
    "            new_images.append(image)\n",
    "\n",
    "    # False\n",
    "    elif image_aspect_ratio == \"anyres\":\n",
    "        for image in images:\n",
    "            image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints)\n",
    "            new_images.append(image)\n",
    "    # False\n",
    "    else:\n",
    "        return image_processor(images, return_tensors='pt')['pixel_values']\n",
    "\n",
    "    # ã™ã¹ã¦ã®ç”»åƒãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶ãŒåŒã˜å ´åˆ\n",
    "    if all(x.shape == new_images[0].shape for x in new_images):\n",
    "\n",
    "        # ã‚¹ã‚¿ãƒƒã‚¯ã—ã¦ãƒãƒƒãƒãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "        new_images = torch.stack(new_images, dim=0)\n",
    "\n",
    "    logger.debug(f\"å‰å‡¦ç†ã•ã‚ŒãŸç”»åƒãƒ†ãƒ³ã‚½ãƒ« {new_images.shape=}, {new_images.dtype=}\")\n",
    "\n",
    "    return new_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eeca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):\n",
    "    \"\"\"\n",
    "    ç”»åƒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "\n",
    "    promptãŒä¾‹ãˆã°ä»¥ä¸‹ã®å ´åˆã€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¯<image>\n",
    "    A chat between... USER: <image>\\nWhat are the things... ASSISTANT: This ...\n",
    "    ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¯-200ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚Œã‚‹\n",
    "\n",
    "    Args:\n",
    "        prompt (str): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—\n",
    "        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        image_token_index (int): ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "        return_tensors (str or None): æˆ»ã‚Šå€¤ã®å½¢å¼ ('pt'ãªã©)ã€‚Noneã®å ´åˆã¯ãƒªã‚¹ãƒˆã§è¿”ã™ã€‚\n",
    "    Returns:\n",
    "        list or torch.Tensor: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"ç”»åƒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ– {prompt=}, {image_token_index=}, {return_tensors=}\")\n",
    "\n",
    "    # 1) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’<image>ã§åˆ†å‰²ã—ã€ãƒãƒ£ãƒ³ã‚¯åŒ–ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "\n",
    "    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split('<image>')]\n",
    "    logger.debug(f\"{prompt_chunks=}\")\n",
    "\n",
    "    def insert_separator(X, sep):\n",
    "        \"ãƒãƒ£ãƒ³ã‚¯é–“ã«ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ï¼ˆç”»åƒã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’æŒ¿å…¥ã™ã‚‹\"\n",
    "        return [\n",
    "            ele for sublist in zip(X, [sep]*len(X)) for ele in sublist\n",
    "        ][:-1]\n",
    "\n",
    "    # 2) æˆ»ã‚Šå€¤ã®æ§‹ç¯‰\n",
    "\n",
    "    # æˆ»ã‚Šå€¤ã‚’åˆæœŸåŒ–\n",
    "    input_ids = []\n",
    "\n",
    "    offset = 0\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ãŒBOSãƒˆãƒ¼ã‚¯ãƒ³ã§å§‹ã¾ã‚‹å ´åˆ\n",
    "    # True\n",
    "    if len(prompt_chunks) > 0 and \\\n",
    "        len(prompt_chunks[0]) > 0 and \\\n",
    "        prompt_chunks[0][0] == tokenizer.bos_token_id:\n",
    "\n",
    "        # BOSãƒˆãƒ¼ã‚¯ãƒ³åˆ†ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ\n",
    "        offset = 1\n",
    "\n",
    "        # BOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’æˆ»ã‚Šå€¤ã«è¿½åŠ \n",
    "        input_ids.append(prompt_chunks[0][0])\n",
    "\n",
    "    # ãƒãƒ£ãƒ³ã‚¯é–“ã«ç”»åƒã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ-200ï¼‰ã‚’æŒ¿å…¥ã—ã€ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "    for x in insert_separator(\n",
    "        prompt_chunks, [image_token_index] * (offset + 1)\n",
    "    ):\n",
    "        # BOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æˆ»ã‚Šå€¤ã«è¿½åŠ \n",
    "        input_ids.extend(x[offset:])\n",
    "\n",
    "    # 3) æˆ»ã‚Šå€¤ã®å½¢å¼ã‚’å¤‰æ›ã—ã¦è¿”ã™\n",
    "\n",
    "    # True\n",
    "    if return_tensors is not None:\n",
    "\n",
    "        # True\n",
    "        if return_tensors == 'pt':\n",
    "            result = torch.tensor(input_ids, dtype=torch.long)\n",
    "            logger.info(f\"ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³åŒ–çµæœã‚’ãƒ†ãƒ³ã‚½ãƒ«ã§è¿”ã™ {result.shape=}, {result.dtype=}\")\n",
    "            return result\n",
    "\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dabd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(args):\n",
    "    \"\"\"\n",
    "    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        args: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®åå‰ç©ºé–“\n",
    "    Returns:\n",
    "        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®æ–‡å­—åˆ—\n",
    "    \"\"\"\n",
    "    logger.info(f\"ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚’å®Ÿè¡Œ {args.model_path=}, {args.model_base=}, {args.model_name=}, {args.query=}, {args.conv_mode=}, {args.image_file=}, {args.sep=}, {args.temperature=}, {args.top_p=}, {args.num_beams=}, {args.max_new_tokens=}\")\n",
    "\n",
    "    # 1) äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚’ç„¡åŠ¹åŒ–\n",
    "    disable_torch_init()\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«åã‚’æŠ½å‡º\n",
    "    # \"liuhaotian/llava-v1.5-7b\" -> \"llava-v1.5-7b\"\n",
    "    model_name = get_model_name_from_path(args.model_path)\n",
    "\n",
    "    # äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ãƒ¢ãƒ‡ãƒ«ã€ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "        args.model_path, # \"liuhaotian/llava-v1.5-7b\"\n",
    "        args.model_base, # None\n",
    "        model_name, # \"llava-v1.5-7b\"\n",
    "    )\n",
    "\n",
    "    # 2) ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŒ¿å…¥\n",
    "\n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒª\n",
    "    # What are the things I should be cautious about when I visit here?\n",
    "    qs = args.query\n",
    "\n",
    "    # ç”»åƒã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "\n",
    "    # å…¥åŠ›ã‚¯ã‚¨ãƒªã«ç”»åƒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãŒå«ã¾ã‚Œã‚‹å ´åˆ\n",
    "    # False\n",
    "    if IMAGE_PLACEHOLDER in qs:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
    "        else:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
    "    # True\n",
    "    else:\n",
    "        # ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç‰¹æ®Šãªé–‹å§‹ãƒ»çµ‚äº†ã‚¿ã‚°ã§å›²ã‚€ã‹\n",
    "        # False\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            qs = image_token_se + \"\\n\" + qs\n",
    "        else:\n",
    "            # ç”»åƒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ã‚¯ã‚¨ãƒªã®å…ˆé ­ã«è¿½åŠ \n",
    "            # <image>\\nWhat are the things...\n",
    "            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
    "\n",
    "    # 3) ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ã‚’æ±ºå®š\n",
    "\n",
    "    # False\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        conv_mode = \"llava_llama_2\"\n",
    "\n",
    "    # False\n",
    "    elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "\n",
    "    # False\n",
    "    elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "\n",
    "    # True\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        # ãƒãƒ£ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’LLaVA v1ã«è¨­å®š\n",
    "        conv_mode = \"llava_v1\"\n",
    "\n",
    "    # False\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        conv_mode = \"mpt\"\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        conv_mode = \"llava_v0\"\n",
    "\n",
    "    # å¯¾è©±ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æŒ‡å®šãŒãªã„å ´åˆ\n",
    "    # False\n",
    "    if args.conv_mode is not None and conv_mode != args.conv_mode:\n",
    "        print(\n",
    "            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "                conv_mode, args.conv_mode, args.conv_mode\n",
    "            )\n",
    "        )\n",
    "    # True\n",
    "    else:\n",
    "        args.conv_mode = conv_mode\n",
    "\n",
    "    logger.debug(f\"ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ã‚’æ±ºå®š {args.conv_mode=}\")\n",
    "\n",
    "    # 4) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰\n",
    "\n",
    "    # ä¼šè©±ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—\n",
    "    conv = conv_templates[args.conv_mode].copy()\n",
    "\n",
    "    # ä¼šè©±ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ¦ãƒ¼ã‚¶ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ \n",
    "    # USER, <image>\\nWhat are the things...\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "\n",
    "    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ \n",
    "    # ASSISTANT, None\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "\n",
    "    # æœ€çµ‚çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    # 5) ç”»åƒã®å‰å‡¦ç†\n",
    "\n",
    "    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "    # [\"https://llava-vl.github.io/static/images/view.jpg\"]\n",
    "    image_files = image_parser(args)\n",
    "\n",
    "    # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€PILç”»åƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—    \n",
    "    images = load_images(image_files)\n",
    "\n",
    "    # ç”»åƒã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "    # [(1000, 667)]\n",
    "    image_sizes = [x.size for x in images]\n",
    "    logger.debug(f\"ç”»åƒã‚µã‚¤ã‚ºã‚’å–å¾— {image_sizes=}\")\n",
    "\n",
    "    # ç”»åƒã‚’å‰å‡¦ç†ã—ã€ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "    images_tensor = process_images(\n",
    "        images,\n",
    "        image_processor,\n",
    "        model.config\n",
    "    ).to(model.device, dtype=torch.float16)\n",
    "\n",
    "    # 6) ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "    # ã“ã®ã¨ãã€<image>ã¯-200ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚Œã‚‹\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(\n",
    "            prompt,\n",
    "            tokenizer,\n",
    "            IMAGE_TOKEN_INDEX,\n",
    "            return_tensors=\"pt\"\n",
    "        ).unsqueeze(0).cuda()\n",
    "    )\n",
    "    logger.debug(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ– {input_ids}\")\n",
    "\n",
    "    # 7) ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=images_tensor,\n",
    "            image_sizes=image_sizes,\n",
    "            do_sample=True if args.temperature > 0 else False,\n",
    "            temperature=args.temperature,\n",
    "            top_p=args.top_p,\n",
    "            num_beams=args.num_beams,\n",
    "            max_new_tokens=args.max_new_tokens,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    # 8) çµæœã®ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "\n",
    "    outputs = tokenizer.batch_decode(\n",
    "        output_ids,\n",
    "        skip_special_tokens=True\n",
    "    )[0].strip()\n",
    "\n",
    "    logger.info(f\"ç”Ÿæˆçµæœã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº† {outputs=}\")\n",
    "\n",
    "prompt = \"What are the things I should be cautious about when I visit here?\"\n",
    "image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "\n",
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"model_base\": None,\n",
    "    \"model_name\": get_model_name_from_path(model_path),\n",
    "    \"query\": prompt,\n",
    "    \"conv_mode\": None,\n",
    "    \"image_file\": image_file,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 512\n",
    "})()\n",
    "\n",
    "eval_model(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
